[
  {
    "objectID": "web_scraping.html",
    "href": "web_scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "This page has information about web scraping."
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Principal Component Analysis uses the following ideas:\n\nextracts variance structure from high dimensional datasets\northogonal projection or transformation of the data into a (possibly lower dimensional) subspace to maximize the variance (thought experiment: how would a 2D being capture a human)\nretains as much information as possible with as little loss as possible\nhelps to deduce whether or not features are independent\ncan make features less interpretable\neach dimension in the dataset results in a principal component (eigenvector)"
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Designed from how our own neurons work, i.e. the human brain!\nSee the Terminology tab for detailed basics."
  },
  {
    "objectID": "neural_networks.html#elements",
    "href": "neural_networks.html#elements",
    "title": "Neural Networks",
    "section": "Elements",
    "text": "Elements\n\nhidden layers\nunits\nactivation function\nobjective function\nweights"
  },
  {
    "objectID": "neural_networks.html#how-do-they-learn",
    "href": "neural_networks.html#how-do-they-learn",
    "title": "Neural Networks",
    "section": "How do They Learn?",
    "text": "How do They Learn?\n\nforward propagation\n\nweights are initialized or updated\ncomputation flows from the input layer to the output layer through hidden layers"
  },
  {
    "objectID": "neural_networks.html#how-are-the-weights-updated",
    "href": "neural_networks.html#how-are-the-weights-updated",
    "title": "Neural Networks",
    "section": "How are the Weights Updated?",
    "text": "How are the Weights Updated?\n\nstochastic gradient descent\ncomputing the gradient of the loss function with respect to each weight through the chain rule\ncomputing the gradient one layer at a time and iterating backward from the output layer to the input layer"
  },
  {
    "objectID": "neural_networks.html#multi-class-neural-networks",
    "href": "neural_networks.html#multi-class-neural-networks",
    "title": "Neural Networks",
    "section": "Multi-Class Neural Networks",
    "text": "Multi-Class Neural Networks\nSoftmax\n\nlogistic regression produces:\n\nprobability between 0 and 1\nthe probability represents the likelihood of the input belong to the class (Y/N)\n\nsoftmax extends this idea into a multi-class world\nsoftmax produces:\n\na set of probabilities between 0 and 1\neach probability represents the likelihood of the input belonging to a particular class"
  },
  {
    "objectID": "neural_networks.html#summary",
    "href": "neural_networks.html#summary",
    "title": "Neural Networks",
    "section": "Summary",
    "text": "Summary\n\nneural networks are nets of layers and nodes\n\nnodes at each layer sum up the weighted inputs and activate it to the next layer\nprovides a way of learning features\nhighly nonlinear prediction functions\n\nbackpropagation is a special case of reverse-mode Automatic Differentiation (AD) and provides an efficient way to compute gradients\nhyperparameters are a new way of learning"
  },
  {
    "objectID": "neural_networks.html#choosing-hidden-layers",
    "href": "neural_networks.html#choosing-hidden-layers",
    "title": "Neural Networks",
    "section": "Choosing (hidden) Layers",
    "text": "Choosing (hidden) Layers\n\n0 layers: can only represent linear separable functions or decisions\n1 layer: universal function approximator\n2 layers: can represent an arbitrary decision boundary"
  },
  {
    "objectID": "neural_networks.html#topology",
    "href": "neural_networks.html#topology",
    "title": "Neural Networks",
    "section": "Topology",
    "text": "Topology\nThe topology of a neural network describes:\n\nHidden layers as parameter\nUnits per hidden layers as a parameter\nOutput: 1 unit per class if more than two classes\nTrial-and-error: different topology, different initial weights\n\nExample: design-build project\n\nfive networks\nthree layers\nthree nodes for the input layer\ntwo nodes for the hidden layer\none node for the output layer"
  },
  {
    "objectID": "frequent_patterns.html",
    "href": "frequent_patterns.html",
    "title": "Frequent Patterns",
    "section": "",
    "text": "We’re trying to find frequent itemsets given a list of sets. Essentially, finding the subsets within these sets that are commonly together.\nTwo prime examples of this are items purchased in transactions and movies watched in a library.\n\n\n\n\nSupport quantifies how often an itemset appears in a dataset (proportion).\nLet \\(I\\) be an itemset, then:\n\\(Support(I) = \\frac{\\text{number of sets containing } I}{\\text{total number of sets}}\\)\nSuppose we have 100 customer’s movie watchlists, and we’re interested in making association rules surrounding movie \\(M\\).\nWe know that movie (or movie set) \\(M\\) is in 10 of the watchlists.\n\\(Support(M) = \\frac{10}{100}\\)\nAdditionally, we could have support for an association rule (\\(Support(A \\rightarrow C)\\)), where \\(A\\) is the antecendent and \\(C\\) is the consequent (both are itemsets), which would look like:\n\\(Support(A \\rightarrow C) = \\frac{\\text{number of sets containing } A \\text{ and } C}{\\text{total number of sets}}\\)\n\n\n\nConfidence quantifies the likelihood an itemset’s consequent occurs given its antecent (i.e. the probability a consequent occurs given its antecent).\nLet \\(A\\) be an itemset’s antecent and \\(C\\) be an itemset’s consequent, then:\n\\(Confidence(A \\rightarrow C) = \\frac{\\text{proportion of sets containing} A \\text{ and } C }{\\text{proportion of sets containing} A}\\)\nNote that the concept of confidence is roughly derived with the help of the concept of conditional probability.\n\n\\(Confidence(A \\rightarrow C) = P(C|A) = \\frac{P(C, A)}{P(A)}\\)\n\nA key difference is that the numerator is is not a true intersection of events, but it contains the every element in itemsets \\(A\\) and \\(C\\). It’s more clear to use the respective definitions of support here.\n\\(Confidence(A \\rightarrow C) = \\frac{Support(A \\cup C)}{Support(A)}\\)\nSuppose we want to recommend a movie (or movie set), \\(M_2\\), and we’re creating rules based on those who have watched the movie (or movie set) \\(M_2\\).\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{\\text{proportion of watchlists containing} M_1 \\text{ and } M_2} {\\text{proportion of watchlists containing} M_1}\\)\n\\(= \\frac{Support(M_1 \\cup M_2)}{Support(M_1)}\\)\n\n\n\nLift assesses the performance of an association rule by quantifying an improvement (or degradation) from the initial prediction, where the initial prediction would be the support of the antecent.\n\\(Lift(C \\rightarrow A) = \\frac{Confidence(A \\rightarrow C)}{Support(C)}\\)\n\\(= \\frac{\\frac{Support(A \\cup C)}{Support(A)}}{Support(C)}\\)\n\\(= \\frac{Support(A \\cup C)}{Support(A)Support(C)}\\)\nSuppose we have a rule suggesting we recommend a movie (or movie set), \\(M_2\\), to those have seen movie(s) \\(M_1\\) (i.e. \\(M_1 \\rightarrow M_2\\)).\nWe gather the following measurements:\n\n\\(Support(M_1) = P(M_1) = 0.4\\)\n\\(Support(M_2) = P(M_2) = 0.1\\)\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{Support(M_1 \\cup M_2)}{Support(M_1)} = 0.175\\)\n\\(Lift(M_1 \\rightarrow M_2) = \\frac{Confidence(M_1 \\rightarrow M_2)}{Support(M_2)} = 1.75\\)\n\n\n\n\nBecause \\(lift((M_1 \\rightarrow M_2)) &gt; 1\\), this rule provides evidence that the suggesting \\(M_2\\) to those who have have seen \\(M_1\\) is better than just suggesting \\(M_2\\).\nIn general, this yields the result that if \\(lift &gt; 1\\), the association rule improves our initial prediction.\n\n\n\nMaximal Itemset: itemset in which none of its supersets are frequent.\n\n\n\nClosed Itemset: itemset in which none of its immediate supersets have the same support count as the itemset, itself.\n\n\n\nk-itemset: itemset which contains k items.\n\nWhen finding frequencies, can refer to each sized frequency set as n-frequent, where n is the number of items in an itemset (or subset).\n\n\n\n\nApriori Property: all non-empty subsets of frequent itemsets must be frequent.\n\nGiven a frequent itemset, all non-empty subsets of the frequent itemset are frequent as well.\n\n\n\n\nAntimonotonicity: If a set cannot pass a test, all its supersets will fail the test as well.\nRoughly the contrapositive of the Apriori Property for association rules.\n\nIf an itemset is infrequent, all its supersets will be infrequent as well.\n\n\n\n\n\n\\(2^n - 1\\), where \\(n\\) is the total number of items in the dataset. This is actually problematic due to the factorial growth per element.\n\nFor example, take a frequent itemset with just 100 elements. The total number of frequent itemsets that it contains is:\n\\(\\sum\\limits_{n = 1}^{1000}{100 \\choose n} = 2^{100} - 1 \\approx 1.27 x 10^{30}\\)"
  },
  {
    "objectID": "frequent_patterns.html#the-basics",
    "href": "frequent_patterns.html#the-basics",
    "title": "Frequent Patterns",
    "section": "",
    "text": "Support quantifies how often an itemset appears in a dataset (proportion).\nLet \\(I\\) be an itemset, then:\n\\(Support(I) = \\frac{\\text{number of sets containing } I}{\\text{total number of sets}}\\)\nSuppose we have 100 customer’s movie watchlists, and we’re interested in making association rules surrounding movie \\(M\\).\nWe know that movie (or movie set) \\(M\\) is in 10 of the watchlists.\n\\(Support(M) = \\frac{10}{100}\\)\nAdditionally, we could have support for an association rule (\\(Support(A \\rightarrow C)\\)), where \\(A\\) is the antecendent and \\(C\\) is the consequent (both are itemsets), which would look like:\n\\(Support(A \\rightarrow C) = \\frac{\\text{number of sets containing } A \\text{ and } C}{\\text{total number of sets}}\\)\n\n\n\nConfidence quantifies the likelihood an itemset’s consequent occurs given its antecent (i.e. the probability a consequent occurs given its antecent).\nLet \\(A\\) be an itemset’s antecent and \\(C\\) be an itemset’s consequent, then:\n\\(Confidence(A \\rightarrow C) = \\frac{\\text{proportion of sets containing} A \\text{ and } C }{\\text{proportion of sets containing} A}\\)\nNote that the concept of confidence is roughly derived with the help of the concept of conditional probability.\n\n\\(Confidence(A \\rightarrow C) = P(C|A) = \\frac{P(C, A)}{P(A)}\\)\n\nA key difference is that the numerator is is not a true intersection of events, but it contains the every element in itemsets \\(A\\) and \\(C\\). It’s more clear to use the respective definitions of support here.\n\\(Confidence(A \\rightarrow C) = \\frac{Support(A \\cup C)}{Support(A)}\\)\nSuppose we want to recommend a movie (or movie set), \\(M_2\\), and we’re creating rules based on those who have watched the movie (or movie set) \\(M_2\\).\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{\\text{proportion of watchlists containing} M_1 \\text{ and } M_2} {\\text{proportion of watchlists containing} M_1}\\)\n\\(= \\frac{Support(M_1 \\cup M_2)}{Support(M_1)}\\)\n\n\n\nLift assesses the performance of an association rule by quantifying an improvement (or degradation) from the initial prediction, where the initial prediction would be the support of the antecent.\n\\(Lift(C \\rightarrow A) = \\frac{Confidence(A \\rightarrow C)}{Support(C)}\\)\n\\(= \\frac{\\frac{Support(A \\cup C)}{Support(A)}}{Support(C)}\\)\n\\(= \\frac{Support(A \\cup C)}{Support(A)Support(C)}\\)\nSuppose we have a rule suggesting we recommend a movie (or movie set), \\(M_2\\), to those have seen movie(s) \\(M_1\\) (i.e. \\(M_1 \\rightarrow M_2\\)).\nWe gather the following measurements:\n\n\\(Support(M_1) = P(M_1) = 0.4\\)\n\\(Support(M_2) = P(M_2) = 0.1\\)\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{Support(M_1 \\cup M_2)}{Support(M_1)} = 0.175\\)\n\\(Lift(M_1 \\rightarrow M_2) = \\frac{Confidence(M_1 \\rightarrow M_2)}{Support(M_2)} = 1.75\\)\n\n\n\n\nBecause \\(lift((M_1 \\rightarrow M_2)) &gt; 1\\), this rule provides evidence that the suggesting \\(M_2\\) to those who have have seen \\(M_1\\) is better than just suggesting \\(M_2\\).\nIn general, this yields the result that if \\(lift &gt; 1\\), the association rule improves our initial prediction.\n\n\n\nMaximal Itemset: itemset in which none of its supersets are frequent.\n\n\n\nClosed Itemset: itemset in which none of its immediate supersets have the same support count as the itemset, itself.\n\n\n\nk-itemset: itemset which contains k items.\n\nWhen finding frequencies, can refer to each sized frequency set as n-frequent, where n is the number of items in an itemset (or subset).\n\n\n\n\nApriori Property: all non-empty subsets of frequent itemsets must be frequent.\n\nGiven a frequent itemset, all non-empty subsets of the frequent itemset are frequent as well.\n\n\n\n\nAntimonotonicity: If a set cannot pass a test, all its supersets will fail the test as well.\nRoughly the contrapositive of the Apriori Property for association rules.\n\nIf an itemset is infrequent, all its supersets will be infrequent as well.\n\n\n\n\n\n\\(2^n - 1\\), where \\(n\\) is the total number of items in the dataset. This is actually problematic due to the factorial growth per element.\n\nFor example, take a frequent itemset with just 100 elements. The total number of frequent itemsets that it contains is:\n\\(\\sum\\limits_{n = 1}^{1000}{100 \\choose n} = 2^{100} - 1 \\approx 1.27 x 10^{30}\\)"
  },
  {
    "objectID": "frequent_patterns.html#outline",
    "href": "frequent_patterns.html#outline",
    "title": "Frequent Patterns",
    "section": "Outline",
    "text": "Outline\nThe key steps of the Apriori Algorithm follow this computation process: - initial iteration: count number of occurrences of each item to form candidate 1-itemsets, \\(C_1\\) - initial minimum support check: compare \\(C_1\\) with the minimum support, removing any occurrence which does not pass the minimum support test, to form frequent 1-itemsets, \\(L_1\\) - future iterations: form candidate k-itemsets, \\(C_k\\) where \\(k &gt; 1\\) - join \\(L_{k-1}\\) with itself such that all permutations are formed with length of \\(k\\) - future iterations: form frequent k-itemsets, \\(L_k\\) where \\(k &gt; 1\\) - compare \\(C_k\\) with the minimum support, removing any occurrence which does not pass the minimum support test - repeat until maximum transaction length from dataset or no candidate or frequent itemsets can be generated\n\nNote that by the Apriori pincipal that all non-empty subsets of the frequent itemset are frequent as well, we can report the final \\((k+1)\\)-length frequent itemsets, as those contain the non-empty frequent itemsets."
  },
  {
    "objectID": "frequent_patterns.html#implementation",
    "href": "frequent_patterns.html#implementation",
    "title": "Frequent Patterns",
    "section": "Implementation",
    "text": "Implementation\n\nCustom Example\n\n\nCode\nfrom itertools import permutations\ndef custom_apriori(transaction_dict, min_sup = 0.5):\n    # get all unique items in transactions\n    freq_items = set()\n    for transaction in transaction_dict:\n        freq_items = freq_items.union(transaction_dict[transaction])\n        \n    # begin with frequent k=1 itemsets\n    # boolean initialized to true for meeting support, will exit when false\n    # initialize frequency dictionary to store results\n    meets_support = True\n    k = 1\n    frequency_dict = {}\n    \n    while meets_support:\n        # create list for each permutation, initializing support count at 0\n        prev_perms = []\n        candidate = []\n        for perm in permutations(freq_items, k):\n            # prevent duplicates since order doesn't matter\n            if set(perm) not in prev_perms:\n                candidate.append([set(perm), 0])\n                prev_perms.append(set(perm))\n\n        # loop through each permutation in candidate\n        # if the intersection of the permutation with a transaction is the permutation\n        # then that indicates it is an itemset within the transaction\n        for itemset in candidate:\n            for transaction in transaction_dict:\n                if itemset[0].intersection(transaction_dict[transaction]) == itemset[0]:\n                    itemset[1] += 1\n        # keep the permutations with support count at least at the minimum support\n        frequency = [itemset[0] for itemset in candidate if (itemset[1] / len(transaction_dict)) &gt;= min_sup]\n        # if the frequency list is empty, there are no more permutations which can meet the minimum support threshold\n        # we can exit it at that case due to antimonotonicity\n        if len(frequency) != 0:\n            # add to frequency dictionary for each round\n            frequency_dict[k] = frequency\n            # extract unique items from new frequency itemsets\n            freq_items = set()\n            for itemset in frequency:\n                freq_items = freq_items.union(itemset)\n            k += 1\n            \n        else:\n            meets_support = False\n            \n    return frequency_dict\n\n\nSuppose we were given the example of\n\n\n\nCode\ntransaction_dict = {'t1': {'M', 'N', 'P', 'X', 'Z'},\n                    't2': {'M', 'A', 'P', 'X', 'O'},\n                    't3': {'D', 'N', 'P', 'X', 'Z'},\n                    't4': {'M', 'O', 'C', 'P', 'Z'},\n                    't5': {'M', 'K', 'I', 'Z', 'O'}}\n\n\nA quick aside on the number of datasets:\n\\(2^n - 1 = 2^{11} - 1 = 2047\\) possible frequent itemsets.\nThe results of the Custom Apriori Algorithm will produce:\n\n\nCode\ncustom_apriori(transaction_dict)\n\n\n{1: [{'P'}, {'Z'}, {'M'}, {'X'}, {'O'}],\n 2: [{'P', 'X'}, {'P', 'Z'}, {'M', 'P'}, {'M', 'O'}, {'M', 'Z'}]}\n\n\nNote that we’re not giving the interim support counts or the final support counts, how about we take a look at a widely used and accepted module.\n\n\nThe mlxtend Library\nImport the following from mlxtend:\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori as ml_apriori\nfrom mlxtend.frequent_patterns import association_rules as ml_association\n\n\nFor the TransactionEncoder, we’re going to want the transactions in a list of lists:\n\n\nCode\n# turn transactions into list of lists\ntransactions = [list(transaction_dict[transaction]) for transaction in  transaction_dict]\n\n\nEncode the transactions list:\n\n\nCode\n# put the transactions' lists in a boolean dataframe format\ncon = TransactionEncoder()\ncon_arr = con.fit(transactions).transform(transactions)\ndf_transformed = pd.DataFrame(con_arr, columns = con.columns_)\n\n\nRun the algorithm:\n\n\nCode\n# mlxtend apriori function\ndataset_apriori = ml_apriori(df_transformed, use_colnames = True, min_support = 0.5)\ndataset_apriori\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.8\n(M)\n\n\n1\n0.6\n(O)\n\n\n2\n0.8\n(P)\n\n\n3\n0.6\n(X)\n\n\n4\n0.8\n(Z)\n\n\n5\n0.6\n(O, M)\n\n\n6\n0.6\n(P, M)\n\n\n7\n0.6\n(Z, M)\n\n\n8\n0.6\n(X, P)\n\n\n9\n0.6\n(P, Z)\n\n\n\n\n\n\n\nAnd then extending past the frequent itemset results, we can run a function which will return association rules:\n\n\nCode\n# mlxtend association rules to display the rules with a 50% minimum threshold on the frequent itemsets (dataset_apriori)\nrules = ml_association(dataset_apriori, metric = 'confidence', min_threshold = 0.50)\nrules\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nzhangs_metric\n\n\n\n\n0\n(O)\n(M)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n1\n(M)\n(O)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n2\n(P)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n3\n(M)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n4\n(Z)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n5\n(M)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n6\n(X)\n(P)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n7\n(P)\n(X)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n8\n(P)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n9\n(Z)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25"
  },
  {
    "objectID": "frequent_patterns.html#partitioning",
    "href": "frequent_patterns.html#partitioning",
    "title": "Frequent Patterns",
    "section": "Partitioning",
    "text": "Partitioning\n\nA frequent itemset must be frequent in at least one partition\nIf an itemset is not frequent in all partitions, we can ignore it\nUsing this, we can divide and conquer by addressing the following:\n\nWhat is the size of each partition?\nWhat is the number of partitions?\n\n\nThe partitioning method takes two data scans."
  },
  {
    "objectID": "frequent_patterns.html#implementation-1",
    "href": "frequent_patterns.html#implementation-1",
    "title": "Frequent Patterns",
    "section": "Implementation",
    "text": "Implementation\nFP-Growth uses hash-trees to find frequent itemsets with candidate generation as mining long patterns needs many scans and generates a lot of candidates. It helps avoid common bottlenecks such as candidate generation and candidate tests.\nFP Tree Construction\n\nScan and find frequent 1-itemset (given a minimum support)\nSort the frequent 1-itemset in descending order\nScan and construct the FP-tree\nFind conditional pattern base"
  },
  {
    "objectID": "frequent_patterns.html#the-mlxtend-library-1",
    "href": "frequent_patterns.html#the-mlxtend-library-1",
    "title": "Frequent Patterns",
    "section": "The mlxtend Library",
    "text": "The mlxtend Library\nImport New Library for FP-Growth.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori as ml_apriori\nfrom mlxtend.frequent_patterns import association_rules as ml_association\n# new function for fp-growth\nfrom mlxtend.frequent_patterns import fpgrowth as ml_fpgrowth\n\n\nWe use the same initialization process as above.\n\n\nCode\ntransactions\n\n\n[['X', 'P', 'N', 'Z', 'M'],\n ['O', 'P', 'X', 'A', 'M'],\n ['X', 'P', 'N', 'Z', 'D'],\n ['O', 'P', 'C', 'Z', 'M'],\n ['O', 'K', 'I', 'Z', 'M']]\n\n\n\n\nCode\ncon = TransactionEncoder()\ncon_arr = con.fit(transactions).transform(transactions)\ndf_transformed = pd.DataFrame(con_arr, columns = con.columns_)\ndf_transformed\n\n\n\n\n\n\n\n\n\nA\nC\nD\nI\nK\nM\nN\nO\nP\nX\nZ\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n3\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\n4\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\nFrequent Itemsets through Apriori\n\n\nCode\n# frequent itemsets through apriori\ndataset_apriori = ml_apriori(df_transformed, use_colnames = True, min_support = 0.5)\ndataset_apriori\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.8\n(M)\n\n\n1\n0.6\n(O)\n\n\n2\n0.8\n(P)\n\n\n3\n0.6\n(X)\n\n\n4\n0.8\n(Z)\n\n\n5\n0.6\n(O, M)\n\n\n6\n0.6\n(P, M)\n\n\n7\n0.6\n(Z, M)\n\n\n8\n0.6\n(X, P)\n\n\n9\n0.6\n(P, Z)\n\n\n\n\n\n\n\nFrequent Itemsets through FP-Growth\n\n\nCode\n# frequent itemsets through fp-growth\ndataset_fpgrowth = ml_fpgrowth(df_transformed, use_colnames = True, min_support = 0.5)\ndataset_fpgrowth\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.8\n(Z)\n\n\n1\n0.8\n(P)\n\n\n2\n0.8\n(M)\n\n\n3\n0.6\n(X)\n\n\n4\n0.6\n(O)\n\n\n5\n0.6\n(P, Z)\n\n\n6\n0.6\n(P, M)\n\n\n7\n0.6\n(Z, M)\n\n\n8\n0.6\n(X, P)\n\n\n9\n0.6\n(O, M)\n\n\n\n\n\n\n\nAssociation Rules through Apriori\n\n\nCode\nrules = ml_association(dataset_apriori, metric = 'confidence', min_threshold = 0.50)\nrules\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nzhangs_metric\n\n\n\n\n0\n(O)\n(M)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n1\n(M)\n(O)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n2\n(P)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n3\n(M)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n4\n(Z)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n5\n(M)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n6\n(X)\n(P)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n7\n(P)\n(X)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n8\n(P)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n9\n(Z)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n\n\n\n\n\nAssociation Rules through FP-Growth\n\n\nCode\nrules = ml_association(dataset_fpgrowth, metric = 'confidence', min_threshold = 0.50)\nrules\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nzhangs_metric\n\n\n\n\n0\n(P)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n1\n(Z)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n2\n(P)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n3\n(M)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n4\n(Z)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n5\n(M)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n6\n(X)\n(P)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n7\n(P)\n(X)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n8\n(O)\n(M)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n9\n(M)\n(O)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n\n\n\n\n\nAside from some minor differences in the output, if we were to sort these results, they would be be very similar (if not identical)."
  },
  {
    "objectID": "cluster_analysis.html",
    "href": "cluster_analysis.html",
    "title": "Cluster Analysis",
    "section": "",
    "text": "Partitioning Criteria:\n\nsingle level\nhierarchical partitioning\noften, multi-level hierarchical partitioning is desirable, i.e. grouping topical terms\n\nSeparation of Clusters:\n\nExclusive: one customer belongs to only one region\nNon-Exclusive: one document may belong to more than one clas\n\nSimilarity Measure:\n\nDistance-Based: Euclidean, road network, vector\nConnectivity-Based: density or contiguity\n\nClustering Space:\n\nFull Space: often when low dimensional\nSubspaces: often in high dimensional clustering"
  },
  {
    "objectID": "cluster_analysis.html#algorithm",
    "href": "cluster_analysis.html#algorithm",
    "title": "Cluster Analysis",
    "section": "Algorithm",
    "text": "Algorithm\n\nPartition objects into k nonempty clusters\nCompute the mean (centroid) of each cluster\nAssign each object to the closest centroid\n\nclosest depends on the distance measurement used (i.e. Euclidean, Manhattan, etc.)\n\nRepeat until there are no more assignment changes"
  },
  {
    "objectID": "cluster_analysis.html#discussion",
    "href": "cluster_analysis.html#discussion",
    "title": "Cluster Analysis",
    "section": "Discussion",
    "text": "Discussion\n\nEfficiency: \\(O(tKn)\\)\n\nn: number of objects\nK: number of clusters\nt: number of iterations\nnormally, \\(K, t &lt;&lt; n\\)\n\nK-means clustering often terminates at a local optimum (initialization can be important to find high-quality clusters)\nNeed to specify K, the number of clusters, in advance\n\nthere are method to automatically determine the “best” K\nin practice, we often run a range of values and select the “best” K value\n\nSensitive to Noisy Data and Outliers\n\nK-medians, K-medoids, etc. can be better equipped to handle outliers\n\nK-means applies only to objects in a continuous n-dimensional space\n\nusing the K-modes for categorical data\n\nNot suitable to discover clusters with non-convex shapes\n\nmore suitable methods are using density-based clustering, kernel K-means, etc.\n\nFurther Variations of K-Means\n\nChoosing better initial centroid estimates\n\nK-means++, Intelligent K-Means, Genetic K-Means\n\nChoosing different representative prototypes for clusters\n\nK-Medoids, K-Medians, K-Modes\n\nApplying feature transformation techniques\n\nWeighted K-Means, Kernel K-Means"
  },
  {
    "objectID": "cluster_analysis.html#discussion-1",
    "href": "cluster_analysis.html#discussion-1",
    "title": "Cluster Analysis",
    "section": "Discussion",
    "text": "Discussion\n\nK-Medoids Clustering: find representative objects (medoids) in clusters\nPAM\n\nstarts from an initital set of medoids\niteratively replaces one of the medoids by one of the non-medoids if it improves the total sum of squared errors (SSE) of the resulting clustering\nPAM works effectively for small data sets but does not scale well for large data sets (due to the computational complexity)\n\nEfficiency Improvements on PAM:\n\nCLARA: PAM on samples\nCLARANS: randomized re-sampling, ensuring efficiency & quality"
  },
  {
    "objectID": "cluster_analysis.html#dendrogram-how-clusters-are-merged",
    "href": "cluster_analysis.html#dendrogram-how-clusters-are-merged",
    "title": "Cluster Analysis",
    "section": "Dendrogram: How Clusters are Merged",
    "text": "Dendrogram: How Clusters are Merged\n\nDendrogram: decompose a set of data objects into a tree of clusters by multi-level nested partitioning\nA clustering of the data objects is obtained by cutting the dendrogram at the desired level, then each connected components forms a cluster"
  },
  {
    "objectID": "cluster_analysis.html#agglomerative-clustering-algorithm",
    "href": "cluster_analysis.html#agglomerative-clustering-algorithm",
    "title": "Cluster Analysis",
    "section": "Agglomerative Clustering Algorithm",
    "text": "Agglomerative Clustering Algorithm\n\nAGNES (AGglomerative NESting)\n\nuse single-link method and the dissimilarity matrix\ncontinuously merge nodes that have the least dissimilarity\neventually all nodes belong to the same cluster\n\nAgglomerative clustering varies on different similarity measures among clusters\n\nsingle link (nearest neighbor)\ncomplete link (diameter)\naverage link (group average)\ncentroid link (centroid similarity)"
  },
  {
    "objectID": "cluster_analysis.html#single-link-vs.-complete-link-in-hierarhcical-clustering",
    "href": "cluster_analysis.html#single-link-vs.-complete-link-in-hierarhcical-clustering",
    "title": "Cluster Analysis",
    "section": "Single Link vs. Complete Link in Hierarhcical Clustering",
    "text": "Single Link vs. Complete Link in Hierarhcical Clustering\nSingle Link (nearest neighbor)\n\nsimilarity between two clusters is the similarity between their most similar (nearest neighbor) members\nlocal similarity-based: emphasizing more on close regions, ignoring the overall structure of the cluster\ncapable of clustering non-elliptical shaped group of objects\nsensitive to noise and outliers\n\n\nComplete Link (diameter)\n\nsimilarity between two clusters is the similarity between their most dissimilar members\nmerge two clusters to form one with the smallest diameter\nnonlocal in behavior, obtaining compact shaped cluster\nsensitive to outliers"
  },
  {
    "objectID": "cluster_analysis.html#agglomerative",
    "href": "cluster_analysis.html#agglomerative",
    "title": "Cluster Analysis",
    "section": "Agglomerative",
    "text": "Agglomerative\n\neach observation starts in its own cluster\nclusteres are successively merged together\nlinkage criteria determines merge strategy\n\nward: minimize the sum of squared differences within clusters\nmax/complete: minimize the max distance within clusters\naverage: minimize the average distance within clusters\nsingle: minimize the minimal distance within clusters\n\n\n\n\nFeature Agglomerative\n\nFeature Agglomerative uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features\ndimensionality reduction tool"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nCode1 + 1\n\n[1] 2"
  },
  {
    "objectID": "classification_regression.html",
    "href": "classification_regression.html",
    "title": "Classification and Regression",
    "section": "",
    "text": "Mostly used in classification, i.e. decision tree classification, and is known as a greedy algorithm.\nA flowchart-like tree structure, where each:\n\nRoot Node: the topmost node in a tree\nInternal Node: denotes a test on an attribute\nBranch: represents an outcome of the test\nLeaf Node (terminal node): holds a class label\n\nIn splitting attributes:\n\nDiscrete Values: directly\nContinuous Values: split_point\nDiscrete Values:\n\nbinary tree\nsplitting_subset\n\n\nAttributes are selected at each node based on a heuristic or statistical measure. An attribute selection measure (splitting rules) is a heuristic (enabling someone to discover or learn something for themselves) for selecting the splitting criterion that “best” separates a given data partition of class-labeled training uples into individual classes.\nSome common selection measures:\n\ninformation gain - Iterative Dichotomiser 3 (ID3): multi-valued attributes\ngain ratio (C4.5): unbalanced splits\nGini impurity: multi-valued, equal-sized & pure partitions, not great when the number of classes is large\n\n\n\nThe purer the dataset:\n\nthe less information we need to remember\nthe easier we can make predictions\n\nEntropy is to measure the impurity of a dataset.\nWe call \\(p_i = \\frac{C_{i,D}}{D}\\) the probability that a tuple belong to class \\(C_i\\), \\(m\\) that number of classes, and \\(v\\) are partitions or subsets.\n\\(Info(D) = - \\sum\\limits_{i=1}^{m} p_i log_2(p_i)\\) (entropy)\n\\(Info_A(D) = \\sum\\limits_{j=1}^{v} \\frac{|D_j|}{|D|} Info(D_j)\\)\n\\(Gain(A) = Info(D) - Info_A(D)\\)\nHere’s what this process looks like by hand:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformation gain is biased towards tests with many outcomes, to overcome this, gain ratio applied a kind of normalization to information gain.\n\\(SplitInfo_A(D) = - \\sum\\limits_{j=1}^{v} \\frac{|D_j|}{|D|} log_2(\\frac{|D_j|}{|D|})\\)\n\\(GainRatio(A) = \\frac{Gain(A)}{SplitInfo_A(D)}\\)\n\n\n\n\\(Gini(D) = 1 - \\sum\\limits_{i=1}^m p_i^2\\)\nand given \\(v\\) partitions (\\(v=2\\) would indicate binary),\n\\(Gini_A(D) = \\sum\\limits_{j=1}^v \\frac{D_j}{D} Gini(D_j)\\)\n\\(\\Delta Gini(A) = Gini(D) - Gini_A(D)\\)"
  },
  {
    "objectID": "classification_regression.html#information-gain",
    "href": "classification_regression.html#information-gain",
    "title": "Classification and Regression",
    "section": "",
    "text": "The purer the dataset:\n\nthe less information we need to remember\nthe easier we can make predictions\n\nEntropy is to measure the impurity of a dataset.\nWe call \\(p_i = \\frac{C_{i,D}}{D}\\) the probability that a tuple belong to class \\(C_i\\), \\(m\\) that number of classes, and \\(v\\) are partitions or subsets.\n\\(Info(D) = - \\sum\\limits_{i=1}^{m} p_i log_2(p_i)\\) (entropy)\n\\(Info_A(D) = \\sum\\limits_{j=1}^{v} \\frac{|D_j|}{|D|} Info(D_j)\\)\n\\(Gain(A) = Info(D) - Info_A(D)\\)\nHere’s what this process looks like by hand:"
  },
  {
    "objectID": "classification_regression.html#gain-ratio",
    "href": "classification_regression.html#gain-ratio",
    "title": "Classification and Regression",
    "section": "",
    "text": "Information gain is biased towards tests with many outcomes, to overcome this, gain ratio applied a kind of normalization to information gain.\n\\(SplitInfo_A(D) = - \\sum\\limits_{j=1}^{v} \\frac{|D_j|}{|D|} log_2(\\frac{|D_j|}{|D|})\\)\n\\(GainRatio(A) = \\frac{Gain(A)}{SplitInfo_A(D)}\\)"
  },
  {
    "objectID": "classification_regression.html#gini-impurity",
    "href": "classification_regression.html#gini-impurity",
    "title": "Classification and Regression",
    "section": "",
    "text": "\\(Gini(D) = 1 - \\sum\\limits_{i=1}^m p_i^2\\)\nand given \\(v\\) partitions (\\(v=2\\) would indicate binary),\n\\(Gini_A(D) = \\sum\\limits_{j=1}^v \\frac{D_j}{D} Gini(D_j)\\)\n\\(\\Delta Gini(A) = Gini(D) - Gini_A(D)\\)"
  },
  {
    "objectID": "classification_regression.html#the-theory",
    "href": "classification_regression.html#the-theory",
    "title": "Classification and Regression",
    "section": "The Theory",
    "text": "The Theory\nBayes’ Theorem\n\\(P(H|X) = \\frac{P(HX)}{P(X)} = \\frac{P(X|H)P(H)}{P(X)}\\)\nThe Classifier\nGiven\n\\(P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)}\\)\n\\(P(X)\\) is constant for all classes, thus we only need to maximize:\n\\(\\frac{P(X|C_i)}{P(C_i)}\\)\n\\(\\rightarrow P(X|C_i) = \\prod_{k=1}^n P(x_k|C_i)\\)\nNaive Assumption: class conditional independence (no dependence between attributes)."
  },
  {
    "objectID": "classification_regression.html#the-zero-probability-problem-laplacian-correction",
    "href": "classification_regression.html#the-zero-probability-problem-laplacian-correction",
    "title": "Classification and Regression",
    "section": "The Zero-Probability Problem (Laplacian Correction)",
    "text": "The Zero-Probability Problem (Laplacian Correction)\nThe prediction requires each conditional probability to be non-zero, if a single \\(P(x_k|C_i) = 0 \\rightarrow P(X|C_i) = \\prod_{k=1}^n P(x_k|C_i) = 0\\).\nWe account for this by applying the Laplacian Correction (Laplacian Estimator), by adding 1 (or a small integer) to each case’s count."
  },
  {
    "objectID": "classification_regression.html#strengths-and-weaknesses",
    "href": "classification_regression.html#strengths-and-weaknesses",
    "title": "Classification and Regression",
    "section": "Strengths and Weaknesses",
    "text": "Strengths and Weaknesses\nStrengths\n\nPerformance: A naive Bayesian classifier has comparable performance with decision tree and selected neural network classifiers.\nIncremental: Each training example can incrementally increase/decrease the probability that a hypothesis is correct - prior knowledge can be combined with observed data.\n\nWeaknesses\n\nAssumption: Attributes conditional independence, therefore loss of accuracy.\n\nWe can deal with dependencies through Bayesian Belief Networks."
  },
  {
    "objectID": "classification_regression.html#selecting-k-the-number-of-neighbors",
    "href": "classification_regression.html#selecting-k-the-number-of-neighbors",
    "title": "Classification and Regression",
    "section": "Selecting k (the number of neighbors)",
    "text": "Selecting k (the number of neighbors)\n\nSmall k: potential for overfitting (high variance, low bias)\nBig k: potential for bringing too many irrelevant points (low variance, high bias)"
  },
  {
    "objectID": "classification_regression.html#important-information",
    "href": "classification_regression.html#important-information",
    "title": "Classification and Regression",
    "section": "Important Information",
    "text": "Important Information\n\nlazy method: doesn’t have a learning phase\nnon-parametric data: no assumption about data\noutputs labels\nobject classified by a plurality vote of its neighbors, with the object being assigned to the class most common among k nearest neighbors (k is positive and normally small)"
  },
  {
    "objectID": "classification_regression.html#k-nn-parameters",
    "href": "classification_regression.html#k-nn-parameters",
    "title": "Classification and Regression",
    "section": "k-NN Parameters",
    "text": "k-NN Parameters\nSince k-NN is relying on the votes neighbors, the parameters are:\n\nk: how many neighbors\ndistance: which distance measure to use\n\nMinkowski distance (\\(L_p\\) norm or p-Norm)\nManhattan distance (\\(L_1\\))\nEuclidean distance (\\(L_2\\))"
  },
  {
    "objectID": "classification_regression.html#bias-prevelant-within-k-nn",
    "href": "classification_regression.html#bias-prevelant-within-k-nn",
    "title": "Classification and Regression",
    "section": "Bias Prevelant within k-NN",
    "text": "Bias Prevelant within k-NN\n\nInductive bias: is the assumption of “nearby means alike” true?\nDistance bias: feature scaling matters\nHigh-Dimension bias: if there are too many dimensions, every data is far away from all others, the meaning of “neighbor” is not valid anymore"
  },
  {
    "objectID": "classification_regression.html#linearly-separable",
    "href": "classification_regression.html#linearly-separable",
    "title": "Classification and Regression",
    "section": "Linearly Separable",
    "text": "Linearly Separable\nSVM turns classification into an optimization problem: find the separating hyperplane with the largest margin possible.\nHard-Margin SVM: an assumption that there is a separating hyperplane.\nSoft-Margin SVM: rather than assume the hyperplane separates ALL data points, we assume it separates the MOST data points.\nSlack Margin: allows us to move points around to make a separating hyperplane possible."
  },
  {
    "objectID": "classification_regression.html#linearly-inseparable",
    "href": "classification_regression.html#linearly-inseparable",
    "title": "Classification and Regression",
    "section": "Linearly Inseparable",
    "text": "Linearly Inseparable\nWhen the data is linearly inseparable, we can transform our data into a higher dimension, then search for the optimal linear separating hyperplane in the new space. Compute dot product on the transformed data mathematically equivalent to applying a kernel function to original data."
  },
  {
    "objectID": "classification_regression.html#overview",
    "href": "classification_regression.html#overview",
    "title": "Classification and Regression",
    "section": "Overview",
    "text": "Overview\n\nClassification for both linear and nonlinear data\nTransforms data to higher dimensions using nonlinear mapping\nSearches for optimal linear separation hyperplanes in a new dimension\nSVM finds this hyperplane using support vectors (“essential” training tuples) and margins (defined by the support vectors)"
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This page has information about data visualization, containing types of plots, use cases, and how to implement using various python libraries."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This website contains knowledge, techniques, and examples on the topic of data mining.\n\nTerminology\nWeb Scraping\nData Visualization\nFrequent Patterns\nClassification and Regression\nCluster Analysis\nPrincipal Component Analysis (PCA)\nOutliers\nBasics of Neural Networks"
  },
  {
    "objectID": "outliers.html",
    "href": "outliers.html",
    "title": "Outliers",
    "section": "",
    "text": "An outlier is a data object that deviates significantly from the rest of the objects, suspected of being generated by a different mechanism."
  },
  {
    "objectID": "outliers.html#detecting-global-outliers",
    "href": "outliers.html#detecting-global-outliers",
    "title": "Outliers",
    "section": "Detecting Global Outliers",
    "text": "Detecting Global Outliers\n\nfind an appropriate measurement of deviation with respect to the application in question\nimportant in many applications\n\nintrusion detection in computer networks\ntrading transaction auditing systems"
  },
  {
    "objectID": "outliers.html#contextual-vs.-behavioral-attributes",
    "href": "outliers.html#contextual-vs.-behavioral-attributes",
    "title": "Outliers",
    "section": "Contextual vs. Behavioral Attributes",
    "text": "Contextual vs. Behavioral Attributes\n\nin contextual outlier detection, the attributes of the data objects in question are divided into two groups\n\ncontextual attributes\nbehavioral attribues\n\nthe contextual attributes of a data object define the object’s context\n\nexample: temperature contextual attributes may be date and location\n\nthe behavioral attributes define the object’s characteristics, and are used to evaluate whether the object is an outlier in the context to which it belongs\n\nexample: temperature behavioral attributes may be temperature, humidity, and pressure\n\nWhether a data object is a contextual outlier depends on not only the behavioral attributes but also the contextual attributes"
  },
  {
    "objectID": "outliers.html#contextual-global-and-local-outliers",
    "href": "outliers.html#contextual-global-and-local-outliers",
    "title": "Outliers",
    "section": "Contextual, Global, and Local Outliers",
    "text": "Contextual, Global, and Local Outliers\n\nglobal outlier detection can be regarded as a special case of contextual outlier detection, where the set of contextual attributes is empty - global outlier detection uses the whole dataset as the context.\ncontextual outlier analysis provideds flexibility to users in that one can examine outliers in different contexts, which can be highly desirable in many applications\nthe quality of contextual outlier detection in an application depends on the meaningfulness of the contextual attributes, in addition to the measurement of the deviation of an object to the majority in the space of behavior attributes\napplications of collective outliers:\n\nintrusion detection, if several computers are sending denial-of-service packages to each other, this could indicate an attack\na large set of transactions of the same stock between a small number of parties in a short period could indicate market manipulation"
  },
  {
    "objectID": "outliers.html#comparison-between-multiple-types-of-outliers",
    "href": "outliers.html#comparison-between-multiple-types-of-outliers",
    "title": "Outliers",
    "section": "Comparison Between Multiple Types of Outliers",
    "text": "Comparison Between Multiple Types of Outliers\n\na dataset can have many types of outliers\nan object may belong to multiple types of outliers\ndifferent applications or purposes could require detection of different types of outliers\nglobal outlier detection is the simplest\ncontext outlier detection requires background information\ncollective outlier detection requires background information to model the relationship among objects to find groups of outliers"
  },
  {
    "objectID": "outliers.html#challenges-of-outlier-detection",
    "href": "outliers.html#challenges-of-outlier-detection",
    "title": "Outliers",
    "section": "Challenges of Outlier Detection",
    "text": "Challenges of Outlier Detection\n\nmodeling normal objects and outliers effectively\n\nthe border between data normality and abnormality (outliers) is often not clear-cut\n\napplication-specific outlier detection\n\nthe relationship among objects highly depends on applications\n\nhandling noise in outlier detection\n\nnoise often unavoidably exists in data collected in many applications\nlow data quality and the presence of noise bring a huge challenge to outlier detection\n\ninterpretability\n\ndetection and understanding of outliers"
  },
  {
    "objectID": "outliers.html#supervised-semi-supervised-and-unsupervised-methods",
    "href": "outliers.html#supervised-semi-supervised-and-unsupervised-methods",
    "title": "Outliers",
    "section": "Supervised, Semi-Supervised, and Unsupervised Methods",
    "text": "Supervised, Semi-Supervised, and Unsupervised Methods\n\nsupervised methods model data normality and abnormality\n\nimbalanced datasets\navoid many false positives in outlier detection\n\nunsupervised methods make an implicit assumption\n\nnormal objects are somewhat clustered\n\nsemi-supervised methods\n\nalthough obtaining some labeled examples is feasible, the number of such labeled examples is often small"
  },
  {
    "objectID": "outliers.html#statistical-methods-proximity-based-methods-and-reconstruction-based-methods",
    "href": "outliers.html#statistical-methods-proximity-based-methods-and-reconstruction-based-methods",
    "title": "Outliers",
    "section": "Statistical Methods, Proximity-based Methods, and Reconstruction-based Methods",
    "text": "Statistical Methods, Proximity-based Methods, and Reconstruction-based Methods\n\nStatistical methods (also known as model-based methods) make assumptions of data normality\n\nExample: detecting outliers using a statistical (Gaussian) model\n\nProximity-based methods assume that an object is an outlier if the nearest neighbors of the object are far away in feature space, that is, the proximity of the object to its neighbors significantly deviates from the proximity of most of the other objects to their neighbors in the same data set\nReconstruction-based methods: matrix-factorization based methods and pattern-based compression methods\n\nnormal data samples often share certain similarities, can often be represented in a more succinct way, compared with their original representation\nwith the succinct representation, we can reconstruct the original representation of the normal samples well\nfor the samples that cannot be reconstructed well by the succinct representation, we flag them as outliers\n\n\nThe overall idea behind statistical methods for outlier detection is to learn a generative model fitting the given dataset, and then identify those objects in low-probability regions of the model as outliers.\nA parametric method assumes that the normal data objects are generated by a parametric distribution with a finite number of parameters \\(\\Theta\\).\n\nthe probability density function of the parametric distribution \\(f(x, \\Theta)\\) gives the probability that object \\(x\\) is generated by the distribution\nthe smaller this value, the more likely \\(x\\) is an outlier\n\nA nonparametric method tries to determine the model from the input data."
  },
  {
    "objectID": "outliers.html#detection-of-univariate-outliers-based-on-normal-distribution",
    "href": "outliers.html#detection-of-univariate-outliers-based-on-normal-distribution",
    "title": "Outliers",
    "section": "Detection of Univariate Outliers Based on Normal Distribution",
    "text": "Detection of Univariate Outliers Based on Normal Distribution\n\nassumption: data are generated from a normal distribution\nlearn the parameters of the normal (Gaussian) distribution from the input data, and identify the points with low probability as outliers\nexample: suppose we have a city’s average temperature values for a single month in the last 10 years:\n\n24.0, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, and 29.4\n\na normal distribution is determined by two parameters\n\nthe mean \\(\\mu\\)\nthe standard deviation \\(\\sigma\\)\n\nuse the maximum likelihood method to estimate the parameters \\(\\mu\\) and \\(\\sigma\\):"
  },
  {
    "objectID": "terminology.html",
    "href": "terminology.html",
    "title": "Terminology",
    "section": "",
    "text": "Data Mining is the process of discovering interesting patterns, models, and other kinds of knowledge from massive amounts of data.\nInterestingness is an either subjective or objective measure to guide the discovery process in data mining in which knowledge from data is judged on being novel, useful, and easily understood by humans."
  },
  {
    "objectID": "terminology.html#data-preparation",
    "href": "terminology.html#data-preparation",
    "title": "Terminology",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nCleaning\nRemoving noisy and inconsistent data.\nData in the real world is messy, and comes with potential for incorrectness whether that comes from instrument failures, human error, or transmission issues. Some errors we’ll find are:\n\nIncomplete Data: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data.\n\nignore the data\ndelete the data (if not too much data loss occurs)\nfill in data manually\nfill in data automatically with methods like averages or modes (fill all, backwards fill, forwards fill, etc.)\npossible smarter technique is to use an inference-based approach (Bayesian)\n\nNoisy: containing noise, errors, or outliers.\n\nRegression: fit data into regression functions\nClustering: detect and remove outliers\nSemi-supervised: detect suspicious values and check via human\n\nInconsistent: containing discrepancies in codes or names.\nIntentional: disguising missing data (i.e. dates either being the first or the last of a given month, or every birthday is January 1).\n\n\n\nIntegration\nMultiple data sources may be combined.\n\n\nTransformations\nData is transformed and consolidated into forms appropriate for mining.\nSome common transformation techniques are:\n\nsmoothing: remove noise from data\naggregation: summarization (i.e. daily \\(\\rightarrow\\) monthly)\ngeneralizatoin: concept hierarchy climbing (i.e. street \\(\\rightarrow\\) city \\(\\rightarrow\\) state)\nnormalization: scale to fall within a range\nattribute/feature construction\nDiscretization: process of converting continuous type data into discrete type data, which reduces the difficulty of data evaluation, management, and mining.\n\n\n\nSelection\nData relevant to the analysis task are retrieved.\n\n\nData Mining\nProcess where intelligent methods are applied to extract patterns or construct models.\n\n\nPattern/Model Evaluation\nIdentify patterns or models representing knowledge based on interestingness measures.\n\n\nKnowledge Presentation\nUsing visualization and knowledge representation techniques to present mined knowledge."
  },
  {
    "objectID": "terminology.html#data-reduction",
    "href": "terminology.html#data-reduction",
    "title": "Terminology",
    "section": "Data Reduction",
    "text": "Data Reduction\nData reduction is used to to reduce the size of the data (less storage, faster processing, etc.). The data is smaller in volume but can produce similary analytical results. This can be relevant for multiple steps in the data mining pipeline. Some method of data reduction are:\n\nregression and log-linear models\nhistograms, clustering, sampling\ndata cube aggregation\ndata compression\nSampling\n\n\nParametric Data Reduction: Regression Analysis\nRegression analysis is a collective name for techniques involving modeling and analysis of numerical data consisting of values of a dependent variable (response variable) and one or more independent variables (predictors). Some common regression analysis techniques are:\n\nlinear regression\nmultiple linear regression\nnonlinear regression\nlog-linear regression\n\n\n\nNon-Parametric Data Reduction:\n\nData Cube Aggregation\nHistogram Analysis\nClustering\n\n\n\nDimensionality Reduction\nThe curse of dimensionality: data becomes increasingly sparse, density and distance between points becomes less meaningful (detrimental for clustering and outlier analysis), and the possible subspaces will grow exponentially.\nDimensionality reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables.\nNot only do dimensionality reduction techniques help to avoid the curse of dimensionality, but also eliminate irrelevant features and reduce noise. Overall, this reduces the time and space complexity required for data mining and allows for easier visualization.\nMethodologies:\n\nfeature selection: find a subset o the original variables.\nfeature extraction: transform the data in the high-dimensional space to a space of fewer dimensions.\n\nA common technique is Principal Component Analysis (PCA). It’s a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The original data are projected onto a much smaller space, resulting in dimensionality reduction."
  },
  {
    "objectID": "terminology.html#record-data",
    "href": "terminology.html#record-data",
    "title": "Terminology",
    "section": "Record Data",
    "text": "Record Data\n\nRelational records or tables (highly structured)\nData matrix, numerical matrix, crosstabs\nTransaction data\nDocument data: term-frequency vector from text documents"
  },
  {
    "objectID": "terminology.html#graphs-networks",
    "href": "terminology.html#graphs-networks",
    "title": "Terminology",
    "section": "Graphs & Networks",
    "text": "Graphs & Networks\n\nTransportation network\nWorld Wide Web\nMolecular Structures\nSocial Information Networks"
  },
  {
    "objectID": "terminology.html#ordered-data",
    "href": "terminology.html#ordered-data",
    "title": "Terminology",
    "section": "Ordered Data",
    "text": "Ordered Data\n\nVideo Data (sequences of images)\nTemporal Data (time series)\nSequential Data (i.e. Transaction Sequences)\nGenetic Sequence Data"
  },
  {
    "objectID": "terminology.html#spatial-image-multimedia-data",
    "href": "terminology.html#spatial-image-multimedia-data",
    "title": "Terminology",
    "section": "Spatial, Image, & Multimedia Data",
    "text": "Spatial, Image, & Multimedia Data\n\nSpatial Data\nImage Data\nVideo Data"
  },
  {
    "objectID": "terminology.html#measures-of-central-tendency",
    "href": "terminology.html#measures-of-central-tendency",
    "title": "Terminology",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\n\nmean: also known as the average, and has some variations:\n\nstandard mean: \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^{n}x_i\\)\nweighted mean: \\(\\bar{x} = \\frac{\\sum\\limits_{i=1}^{n}x_i w_i}{\\sum\\limits_{i=1}^{n}w_i}\\)\ntrimmed mean: removing outliers (extreme values) before calculating the mean\n\nmedian: middle-most value of ordered set (exact middle if odd numbered set, average of the middle if even numbered set)\nmode: most frequent value(s) in a set\nmidrange: \\(\\frac{\\text{minimum + maximum}}{2}\\)\n\nThey each have their importance.\nThe mean is one of the more common measures to describe a dataset. For a dataset which is symmetric, it is the center of the distribution. We use a weighted mean when each value has a relevant associated significance, importance, or occurence frequency. The trimmed mean is useful because even a small number of extreme occurences can corrupt the mean. We can remove small portions of the top and bottom of an ordered dataset, but should avoid removing too much as that can lead to unwanted information loss.\nExamples of when the standard form of mean is used is when comparing ages for a dataset or grades on an exam. A prime example for weighted means is calculating grade point average (GPA), where a grade might be a decimal, but is weighted by the number of credits it is worth.\nFor skewed datasets, or if it’s desired to retain outliers, the median is a better measure of the center of the data.\nSince trimmed means and medians are both good to combat the effect of skew and outliers, a decent example for both of these could be when calculating metrics from a dataset containing salaries. Image we have values with a median around $100,000, but there are just a few values upwards of $500,000. This could significantly alter the standard form of average, but if we trim off these high (and/or low) outliers, and then take the trimmed mean we might see it closer to the median.\nThe mode is useful for both qualitative and quantitative attributes, and represents the highest frequency of an attribute. There can be multiple modes, in which case the data is multimodal (unimodal, bimodal, trimodal for 1, 2 and 3 modes). Or, if each value only occurs once, there is no mode! A good visual for representing mode can be a bar chart. Imagine you’re trying to find the day of the week you have the most customers, the day the highest frequency of customers come in, which day has the tallest bar on the bar chart?\nFor a perfectly symmetric curve, the mean, median, and mode are all at the center of the distribution.\nThe midrange is actually another type of mean that can make for a quick statistic in a dataset. However, for asymmetric data, or data with extreme outliers, it’s unreliable."
  },
  {
    "objectID": "terminology.html#measures-of-dispersion",
    "href": "terminology.html#measures-of-dispersion",
    "title": "Terminology",
    "section": "Measures of Dispersion",
    "text": "Measures of Dispersion\n\nRange: the difference between the maximum and minimum values\nQuartiles:\n\n\\(Q_1\\): \\(25^{th}\\) percentile of data (point at which 25% of the data lies to the left on the distribution)\n\\(Q_2\\): \\(50^{th}\\) percentile of data (also known as the median)\n\\(Q_3\\): \\(75^{th}\\) percentile of data\n\nVariance & Standard Deviation: measure of how much the values deviate from the mean, or simply put, how spread apart the data is. The Standard Deviation is the square root of the Variance. They can be calculated in the following manner:\n\nPopulation Variance: \\(Var(X) = \\frac{1}{N} \\sum\\limits_{i=1}^{N}(x_i - \\mu)^2\\)\nSample Variance: \\(Var(X) = \\frac{1}{n-1} \\sum\\limits_{i=1}^{n}(x_i - \\mu)^2\\)\nIt can also be shown: \\(Var(X) = E[X^2] - E[X]^2\\)\n\\(SD(X) = \\sqrt{Var(X)}\\)\n\nInterquartile Range: the range covered by the middle half of the data, calculated by \\(IQR = Q_3 - Q_1\\)\n\nRange, Quartiles, Variance, Standard Deviation, and the Interquartile Range are known as the five-number summary, and together make up a clearer picture of the data. A rule of thumb about outliers is that a suspected outlier is anything at least \\(1.5 x IQR\\) above \\(Q_3\\) or below \\(Q_1\\).\nOften times, it’s helpful to consider all of these metrics together to start forming hypotheses about data. A good way to visualize the five-number summary is with a box plot."
  },
  {
    "objectID": "terminology.html#similarity-and-dissimilarity",
    "href": "terminology.html#similarity-and-dissimilarity",
    "title": "Terminology",
    "section": "Similarity and Dissimilarity",
    "text": "Similarity and Dissimilarity\n\nDissimilarity Matrix: given a data matrix containing \\(n\\) objects with \\(p\\) attributes, a dissimilarity matrix stores a collection of proximities for all pairs of \\(n\\) objects (usually an \\(nxn\\) table)\n\n\\(d(i, j)\\) is the distance between objects \\(i\\) and \\(j\\)\n\\(d(i, i) = 0\\)\n\\(d(i, j) = d(j, i)\\)\n\nIf \\(d(i, j)\\) is a measure of dissimilarity, then a measure of similarity is \\(sim(i, j) = 1 - d(i, j)\\)\nDissimilarity for Nominal Attributes: \\(d(i, j) = \\frac{p-m}{p}\\); \\(p\\) is the total attributes and \\(m\\) are the matches (weights can be assigned to \\(m\\) as well. This also implies \\(sim(i, j) = 1 - d(i, j) = \\frac{m}{p}\\)\nProximity Measures for Binary Attributes: notably the Jaccard coefficient is used for this. If we let:\n\n\\(q\\) be the number of attributes that both objects have\n\\(r\\) the number of attributes that object \\(i\\) has but not object \\(j\\)\n\\(s\\) the number of attributes that object \\(j\\) has but not object \\(i\\)\n\\(t\\) the number of attributes that neither object has, then…\nJaccard coefficient \\(= sim(i, j) = \\frac{q}{q+r+s} = 1 - d(i, j)\\)\n\nDissimilarity of Numeric Data: Minkowski Distance (Manhattan, Euclidean, etc.). The Minkowski distance generalizes the Manhattan, Euclidean and higher order distance measurements:\n\nManhattan: \\(d(i, j) = \\sum\\limits_{n=1}^{p}|x_{in} - x_{jn}|\\)\nEuclidean: \\(d(i, j) = \\sqrt{\\sum\\limits_{n=1}^{p} (x_{in} - x_{jn})^2}\\)\nMinknowski Generalization: \\(d(i, j) = (\\sum\\limits_{n=1}^{p} |x_{in} - x_{jn}|)^{\\frac{1}{h}}\\)\n\nThere are a plethora of other variations as well… included dissimilarity functions for Ordinal and Mixed attributes, and even a measure of similarity between documetns which used frequency vectors to produce Cosine Similarities\n\nPerhaps the measure listed above most people are familiar with is the Euclidean Distance. This was likely learned in high school, and it’s still a very common method to measure distances between points.\nFor a single example, these methods can be useful in finding clusters. Suppose a dataset isn’t showing any particular linear pattern. What happens if we set a maximum distance between points to be considered a certain family? Clusters may form."
  },
  {
    "objectID": "terminology.html#data-warehouse",
    "href": "terminology.html#data-warehouse",
    "title": "Terminology",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nA data warehouse is a decision support database maintained separately from an organization’s operational database.\nDatabase warehouses support information processing by providing a solid platform of consolidated, historical data for analysis.\n“It is a subject-oriented, integrated, time-variant, and nonvolatile collection of data that support management’s decision-making process.” - William H. Inmon\n\nSubject Oriented\n\nOrganized around major subjects\nFocus on modeling and analyzing data for decision making, not on daily operations or transaction processing.\nExcluding data that are not useful in decision support process provides a simple and concise view of a particular subject issue.\n\n\n\nIntegrated\n\nIntegrate multiple, heterogeneous data sources (relational databases, flat files, online transaction records)\nData cleaning and data integration techniques were applied\nEnsure consistency in naming, encoding, attribute measures, etc.\n\n\n\nTime-Variant\n\nSignificantly longer time span\n\noperational database: current data or recent data\ndata warehouse: historical perspective\n\nEvery key structure in a data warehouse\n\ncontains time information, explicitly or implicitly\nkey of operational data may not contain time information\n\n\n\n\nNon-Volatile\n\nA physically separate store of data transformed from operational environments\nNo operational update of data (no transaction processing, recovery, concurrency control)\nOnly two operations in data accessing\n\nInitial Loading of data\nAccess of data\n\n\n\n\nHow do Data Warehouses Differ from Operational DBMS\n\nOLTP (online transaction processing)\n\nmajor task of traditional relational DBMS (3NF expected)\nday-to-day operations\nclerk, IT professionals, etc.\nusually thousands of users\napplication oriented\ncurrent, up-to-date, flat relational isolated\nsmallish records accessed (GBs)\n\nOLAP (online analytical processing)\n\nmajor task of the data warehouse system\ndata analysis and decision making\nknowledge analysis\nusually hundreds of users\nsubject oriented\nhistorical, summarized, multidimensional, integrated, consolidated\nmillions of records accessed (TBs)\n\n\nHowever, we usually need them both!"
  },
  {
    "objectID": "terminology.html#data-cube",
    "href": "terminology.html#data-cube",
    "title": "Terminology",
    "section": "Data Cube",
    "text": "Data Cube\nA data cube allows for data to be modeled and viewed in multiple dimensions."
  },
  {
    "objectID": "terminology.html#conceptual-modeling",
    "href": "terminology.html#conceptual-modeling",
    "title": "Terminology",
    "section": "Conceptual Modeling",
    "text": "Conceptual Modeling\n\nStar Schema\nA fact table, a set of dimension tables.\n\n\nSnowflake Schema\nA fact table, a hierarchy of dimension tables.\n\n\nFact Constellations\nMultiple fact tables share dimension tables.\n\n\nOLAP Operations\nThe data cube allows for many operations, most notably:\n\ndrilling up (city \\(rightarrow\\) state)\ndrilling down (state \\(rightarrow\\) city)\nslice: project with one dimension\ndice: project with multiple dimensions (sub-cube)\npivot (rotate): visualization, 3D to 2D\n\n\n\nIceberg Cube\nAnalogy: Only a small portion may be above the water in a sparse cube.\nWe compute only the cuboid cells whose aggregate (i.e. count) is above a threshold (a minimum support).\nAvoid explosive growth of the cube."
  },
  {
    "objectID": "terminology.html#the-basics",
    "href": "terminology.html#the-basics",
    "title": "Terminology",
    "section": "The Basics",
    "text": "The Basics\n\nPattern: A set of items, subsequences, or substructures that occur frequently together (or are strongly correlated) in a data set. Patterns represent intrinsic and important properties of datasets.\n\nPattern Discovery is uncovering these structures from massive data sets, and is the foundation for many essential data mining tasks, such as:\n\nassociation, correlation, and causality analysis\nmining sequential structural patterns\nclassification: discriminative pattern-based analysis\ncluster analysis: pattern-based subspace clustering\n\nAnd features broad applications, such as:\n\nmarket basket analysis, cross-marketing, catalog design, sale campaign analysis, web log analysis, biological sequence analysis\nmany types of data: spatiotemporal, multimedia, time-series, and steam data\n\nSee the Frequent Patterns tab for more detailed information."
  },
  {
    "objectID": "terminology.html#supervised-vs.-unsupervised",
    "href": "terminology.html#supervised-vs.-unsupervised",
    "title": "Terminology",
    "section": "Supervised vs. Unsupervised",
    "text": "Supervised vs. Unsupervised\n\nSupervised Learning: Usually used in most classification and regression models. Model built on training data accompanied by known class labels, and new data (testing model/what model is built for) is classified on.\nUnsupervised Learning: Usually used in clustering algorithms, where class labels are unknown. Aims to establish the existence of classes or clusters in the data.\n\nA model can be represented as decision trees, rules, mathematical formulas, or other forms."
  },
  {
    "objectID": "terminology.html#model-validation-and-testing",
    "href": "terminology.html#model-validation-and-testing",
    "title": "Terminology",
    "section": "Model Validation and Testing",
    "text": "Model Validation and Testing\nTest: Using a testing set independent of the training set, we can estimate different model performance metrics (accuracy is the most well known).\nValidation: If the test set is used to select or refine models, it known as a validation (or development) set.\nDeployment: If the accuracy (or other validation metric(s)) is acceptable, use the model to classify new data.\nEvaluation Criteria:\n\nAccuracy: classifcation vs. prediction\nSpeed: time to construct/use the model\nRobustness: handling noise and missing values\nScalability: extension to large amounts of data\nInterpretability: understanding and insight\nGoodness of Rules: decision tree size, compactness of classification rules\n\nCommon Metrics:\n\nAccuracy: the ratio of correct predictions to all predictions\nPrecision: the ratio of true positives to the total number of positives (a measure of exactness), accuracy of the predicted positive\nRecall (sensitivity): the ratio of true positives to the number of total correct predictions (a measure of completeness), true positive recognition\nSpecificity: the ratio of true negatives to the number of total number of negatives (true negative recognition rate), accuracy of the actual negative\nF1-Score: the harmonic mean between precision and recall (a balanced combination, or equal weight, of both precision and recall)"
  },
  {
    "objectID": "terminology.html#extended-evaluation-and-selection",
    "href": "terminology.html#extended-evaluation-and-selection",
    "title": "Terminology",
    "section": "Extended Evaluation and Selection",
    "text": "Extended Evaluation and Selection\nAccuracy\n\nholdout method\ncross-validation\nbootstrap\n\nComparing Classifiers\n\nROC Curves\nConfusion Matrix: shows how the classification model is confused when it makes predictions\n\n\nMetrics in Formula Form\n\n\\(Accuracy = \\frac{TP + TN}{P + N} = sensitivty \\frac{P}{P+N} + specificity \\frac{N}{P+N}\\)\n\\(Error Rate = \\frac{FP + FN}{P + N}\\)\n\\(Precision = \\frac{TP}{TP + FP}\\)\n\\(Specificity = \\frac{TN}{N}\\)\n\\(Recall = Sensitivity = \\frac{TP}{P} = \\frac{TP}{TP+ FN}\\)\n\\(F = \\frac{2*precision*recall}{precision+recall}\\)\n\\(F_{\\beta} = \\frac{(1+\\beta^2)*precision*recall}{\\beta^2*precision+recall}\\)\n\nThis is where F1-Score comes from, by letting \\(\\beta=1\\)\n\\(F_{1} = \\frac{(1+1^2)*precision*recall}{1^2*precision+recall} = F\\)"
  },
  {
    "objectID": "terminology.html#sensitivity-recall-and-precision-tradeoff",
    "href": "terminology.html#sensitivity-recall-and-precision-tradeoff",
    "title": "Terminology",
    "section": "Sensitivity (Recall) and Precision Tradeoff",
    "text": "Sensitivity (Recall) and Precision Tradeoff"
  },
  {
    "objectID": "terminology.html#roc-curve",
    "href": "terminology.html#roc-curve",
    "title": "Terminology",
    "section": "ROC Curve",
    "text": "ROC Curve\nKnown as the receiver operating characteristics curve (ROC curve), and compares true positive rates with false positive rates.\n\nthreshold for classifying positive cases\nhigher the threshold \\(\\rightarrow\\) lower the false positive rate, and lower the true positive rate\nlower the threshold \\(\\rightarrow\\) higher the false positive rate, and higher the true positive rate\nwhen we move the threshold from low to high, this gives a curve of FPR and TPR\n\n\nArea under the curve (AUC):\n\nPerfect: \\(AUC = 1\\)\nWorst: \\(AUC = 0\\)\nRandom: \\(AUC = 0.5\\)"
  },
  {
    "objectID": "terminology.html#full-metrics-example",
    "href": "terminology.html#full-metrics-example",
    "title": "Terminology",
    "section": "Full Metrics Example",
    "text": "Full Metrics Example"
  },
  {
    "objectID": "terminology.html#greedy-algorithms-vs.-dynamic-programming",
    "href": "terminology.html#greedy-algorithms-vs.-dynamic-programming",
    "title": "Terminology",
    "section": "Greedy Algorithms vs. Dynamic Programming",
    "text": "Greedy Algorithms vs. Dynamic Programming\nGreedy Algorithm:\n\nmakes locally optimal choices at each step with the hope of finding a global optimum\ndoes not necessarily consider the future consequences of the current choice\ngenerally faster and simpler than dynamic programming\n\nIn other words, greedy algorithms make a series of localized decisions without considering the bigger picture to find a global solution.\nDynamic Programming:\n\nsolves subproblems recursively, guarantees optimal solution\nslower and more complex than greedy algorithms\n\nIn other words, dynamic programming take a hosistic approach, breaks a problem down into smaller, overlapping subproblems and stores their solutions for future reference. Ensures the overall solution is optimal."
  },
  {
    "objectID": "terminology.html#lazy-vs.-eager-learning",
    "href": "terminology.html#lazy-vs.-eager-learning",
    "title": "Terminology",
    "section": "Lazy vs. Eager Learning",
    "text": "Lazy vs. Eager Learning\n\nLazy: instance based learning, simply stores the training data and waits until it is given a test tuple\n\nless time in training but more time in predicting\neffectively uses a richer hypothesis space since it uses many local linear functions to form an implicit global approximation to the target function\ninstance-based: stores training examples and delays the processing (i.e. lazy)\ntypical approaches:\n\nk-nearest neighbor: instances represented as points in a Euclidean space\nlocally weighted regression: constructs local approximation\ncase-based reasoning: uses symbolic representations and knowledge-based inference\n\n\nEager: given a set of training tuples, constructs a classification model before receiving new data to classify\n\nmore time in training but less time in predicting\nmust commit to a single hypothesis that covers the entire instance space\n\n\nSee the Classification tab for more detailed information."
  },
  {
    "objectID": "terminology.html#techniques",
    "href": "terminology.html#techniques",
    "title": "Terminology",
    "section": "Techniques",
    "text": "Techniques\n\nFeature Elimination\n\nreduce the feature space by eliminating certain features\ndrop features (lost forever)\ndo not modify existing features\n\nFeature Extraction\n\ncombine features into new independent features and have them sorted by importance\n\nmeasurement: how much variance new feature can explain the old feature\n\neliminate the least important feature"
  },
  {
    "objectID": "terminology.html#perceptron",
    "href": "terminology.html#perceptron",
    "title": "Terminology",
    "section": "Perceptron",
    "text": "Perceptron\n\nAn algorithm that learns weights from features.\nIf we consider learning weights as separating a space into two halves by a straight line, the perceptron can be seen as explicitly finding a good linear decision boundary.\nInspired by neurons, the weights are converted to a signal called activation.\nThe activation determines the outcome.\nClassic learning algorithm for the neural model of learning.\nCredibly simple and works well.\nAdvantages:\n\nOnline: rather than considering the entire dataset at once, it only ever looks at one example.\nError-driven: as long as it is doing well, it doesn’t update.\n\nThe algorithm:\n\nLearns one example from another\nIf it makes a good guess, it continues. Otherwise, it updates the parameter and continues\nThe learning rate determines how much to update\nLooping for several iterations\n\nMaxIter defiens how many iterations at most"
  },
  {
    "objectID": "terminology.html#neuron",
    "href": "terminology.html#neuron",
    "title": "Terminology",
    "section": "Neuron",
    "text": "Neuron\nA hidden/output layer unit."
  },
  {
    "objectID": "terminology.html#neural-network",
    "href": "terminology.html#neural-network",
    "title": "Terminology",
    "section": "Neural Network",
    "text": "Neural Network\n\nConnected input/output units\nWeighted connections\nMuli-layer\nFeed-forward\nFully connected\nBackpropagation\nAdjust weights"
  },
  {
    "objectID": "terminology.html#network-topology",
    "href": "terminology.html#network-topology",
    "title": "Terminology",
    "section": "Network Topology",
    "text": "Network Topology\n\nHidden layers as parameter\nUnits per hidden layers as a parameter\nOutput: 1 unit per class if more than two classes\nTrial-and-error: different topology, different initial weights"
  },
  {
    "objectID": "terminology.html#backpropagation",
    "href": "terminology.html#backpropagation",
    "title": "Terminology",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nInitialize weights biases: small random numbers\nPropagate the inputs forward\nBackpropagate the error\nTerminating condition"
  },
  {
    "objectID": "terminology.html#survivorship-bias",
    "href": "terminology.html#survivorship-bias",
    "title": "Terminology",
    "section": "Survivorship Bias",
    "text": "Survivorship Bias\nSelection bias that focuses on the survivors in evaluating an event or outcome.\nSee the Neural Networks tab for more detailed information."
  },
  {
    "objectID": "classification_regression.html#import-libraries-and-data",
    "href": "classification_regression.html#import-libraries-and-data",
    "title": "Classification and Regression",
    "section": "Import Libraries and Data",
    "text": "Import Libraries and Data\n\n\nCode\n# regular libraries\nimport numpy as np\nimport pandas as pd\n\n# import general sklearn libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n\n# import specific sklearn libraries\n\n# decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n# sklearn logistic regression\nfrom sklearn.linear_model import LogisticRegression\n# sklearn knn\nfrom sklearn.neighbors import KNeighborsClassifier \n# sklearn svm (could import svm and use svm.SVC, but we'll directly import SVC)\nfrom sklearn.svm import SVC\n# sklearn naive bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\n\nCode\n# load data\ndf = pd.read_csv('data/diabetes.csv')\n\n# separate predictor and response\nX = df.drop('Outcome', axis = 1)\ny = df['Outcome']\n\n# separate train and test set\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.30, random_state = 42)"
  },
  {
    "objectID": "classification_regression.html#decision-tree",
    "href": "classification_regression.html#decision-tree",
    "title": "Classification and Regression",
    "section": "Decision Tree",
    "text": "Decision Tree\n\n\nCode\n# train data\nclf = DecisionTreeClassifier().fit(X_train, y_train)\n\n# validate model through testing\ny_pred = clf.predict(X_test)\n\n# calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Test Accuracy: {accuracy}')\nprint(f'Test Precision: {precision}')\nprint(f'Test Recall: {recall}')\nprint(f'Test F1: {f1}')\nprint(f'Test Confusion Matrix: \\n{confusion_matrix}')\n\n\nTest Accuracy: 0.7056277056277056\nTest Precision: 0.5612244897959183\nTest Recall: 0.6875\nTest F1: 0.6179775280898877\nTest Confusion Matrix: \n[[108  43]\n [ 25  55]]"
  },
  {
    "objectID": "classification_regression.html#logistic-regression-1",
    "href": "classification_regression.html#logistic-regression-1",
    "title": "Classification and Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nNote that logistic regression works better with array type data in non-scaled data, but ultimately performs better with scaled data.\n\nNon-Scaled Data\nApply array to data:\n\n\nCode\nX_train_array = np.array(X_train)\nX_test_array = np.array(X_test)\ny_train_array = np.array(y_train)\ny_test_array = np.array(y_test)\n\n\n\n\nCode\n# train data\nclf = LogisticRegression().fit(X_train_array, y_train_array)\n\n# validate model through testing\ny_pred = clf.predict(X_test_array)\n\n# calculate metrics\naccuracy = accuracy_score(y_test_array, y_pred)\nprecision = precision_score(y_test_array, y_pred)\nrecall = recall_score(y_test_array, y_pred)\nf1 = f1_score(y_test_array, y_pred)\n\nprint(f'Test Accuracy: {accuracy}')\nprint(f'Test Precision: {precision}')\nprint(f'Test Recall: {recall}')\nprint(f'Test F1: {f1}')\n\n\nTest Accuracy: 0.7402597402597403\nTest Precision: 0.625\nTest Recall: 0.625\nTest F1: 0.625\n\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n\n\nScaled Data\nScale Data\n\n\nCode\nscaler = StandardScaler()\nX_train_normal = scaler.fit_transform(X_train)\nX_test_normal = scaler.fit_transform(X_test)\n\n\n\n\nCode\n# train data\nclf = LogisticRegression().fit(X_train_normal, y_train)\n\n# validate model through testing\ny_pred = clf.predict(X_test_normal)\n\n# calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Test Accuracy: {accuracy}')\nprint(f'Test Precision: {precision}')\nprint(f'Test Recall: {recall}')\nprint(f'Test F1: {f1}')\n\n\nTest Accuracy: 0.7445887445887446\nTest Precision: 0.64\nTest Recall: 0.6\nTest F1: 0.6193548387096774"
  },
  {
    "objectID": "classification_regression.html#k-nearest-neighbors-knn",
    "href": "classification_regression.html#k-nearest-neighbors-knn",
    "title": "Classification and Regression",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\n\nCode\n# train data\nclf = KNeighborsClassifier().fit(X_train, y_train)\n\n# validate model through testing\ny_pred = clf.predict(X_test)\n\n# calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Test Accuracy: {accuracy}')\nprint(f'Test Precision: {precision}')\nprint(f'Test Recall: {recall}')\nprint(f'Test F1: {f1}')\n\n\nTest Accuracy: 0.6883116883116883\nTest Precision: 0.5487804878048781\nTest Recall: 0.5625\nTest F1: 0.5555555555555556"
  },
  {
    "objectID": "classification_regression.html#support-vector-machines-svm",
    "href": "classification_regression.html#support-vector-machines-svm",
    "title": "Classification and Regression",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\n\n\nCode\n# train data\nclf = SVC().fit(X_train, y_train)\n\n# validate model through testing\ny_pred = clf.predict(X_test)\n\n# calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Test Accuracy: {accuracy}')\nprint(f'Test Precision: {precision}')\nprint(f'Test Recall: {recall}')\nprint(f'Test F1: {f1}')\n\n\nTest Accuracy: 0.7359307359307359\nTest Precision: 0.6610169491525424\nTest Recall: 0.4875\nTest F1: 0.5611510791366906\n\n\n\nConfusion Matrix wasn’t performing properly in a few methods above, and was thus removed for the purposes of this website."
  },
  {
    "objectID": "classification_regression.html#gridsearchcv",
    "href": "classification_regression.html#gridsearchcv",
    "title": "Classification and Regression",
    "section": "GridSearchCV",
    "text": "GridSearchCV\nGridSearchCV can be used to test multiple parameters at once.\n\n\nCode\n# decision tree parameters to run through\ntree_parameters = {'criterion': ('gini', 'entropy', 'log_loss'),\n              'splitter': ('best', 'random'),\n              'max_depth': (None, 2, 4, 6, 8, 10),\n              'max_features': (None, 'sqrt', 'log2'),\n              'class_weight': (None, 'balanced')}\n              \n# create gridsearchcv object\ntree_clf_hyper = GridSearchCV(DecisionTreeClassifier(), tree_parameters)\n\n# train the model\ntree_clf_hyper.fit(X_train, y_train)\n\n# results\ntree_clf_hyper_results = pd.DataFrame(tree_clf_hyper.cv_results_)\ntree_clf_hyper_results\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_class_weight\nparam_criterion\nparam_max_depth\nparam_max_features\nparam_splitter\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.004593\n0.000333\n0.001318\n0.000481\nNone\ngini\nNone\nNone\nbest\n{'class_weight': None, 'criterion': 'gini', 'm...\n0.722222\n0.675926\n0.775701\n0.728972\n0.654206\n0.711405\n0.042638\n74\n\n\n1\n0.002844\n0.000766\n0.001678\n0.000568\nNone\ngini\nNone\nNone\nrandom\n{'class_weight': None, 'criterion': 'gini', 'm...\n0.620370\n0.685185\n0.691589\n0.644860\n0.719626\n0.672326\n0.035292\n171\n\n\n2\n0.002722\n0.000609\n0.001607\n0.000323\nNone\ngini\nNone\nsqrt\nbest\n{'class_weight': None, 'criterion': 'gini', 'm...\n0.731481\n0.675926\n0.700935\n0.672897\n0.682243\n0.692696\n0.021701\n135\n\n\n3\n0.002299\n0.000398\n0.001002\n0.000317\nNone\ngini\nNone\nsqrt\nrandom\n{'class_weight': None, 'criterion': 'gini', 'm...\n0.638889\n0.666667\n0.682243\n0.710280\n0.710280\n0.681672\n0.027176\n155\n\n\n4\n0.002841\n0.000331\n0.001354\n0.000401\nNone\ngini\nNone\nlog2\nbest\n{'class_weight': None, 'criterion': 'gini', 'm...\n0.694444\n0.703704\n0.719626\n0.728972\n0.710280\n0.711405\n0.012037\n72\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n211\n0.003445\n0.001160\n0.001827\n0.000417\nbalanced\nlog_loss\n10\nNone\nrandom\n{'class_weight': 'balanced', 'criterion': 'log...\n0.694444\n0.657407\n0.728972\n0.728972\n0.785047\n0.718969\n0.042327\n52\n\n\n212\n0.003373\n0.000604\n0.001410\n0.000393\nbalanced\nlog_loss\n10\nsqrt\nbest\n{'class_weight': 'balanced', 'criterion': 'log...\n0.685185\n0.740741\n0.635514\n0.728972\n0.757009\n0.709484\n0.043994\n83\n\n\n213\n0.002279\n0.000628\n0.001299\n0.000381\nbalanced\nlog_loss\n10\nsqrt\nrandom\n{'class_weight': 'balanced', 'criterion': 'log...\n0.685185\n0.675926\n0.719626\n0.616822\n0.635514\n0.666615\n0.036574\n182\n\n\n214\n0.003816\n0.000370\n0.001796\n0.000472\nbalanced\nlog_loss\n10\nlog2\nbest\n{'class_weight': 'balanced', 'criterion': 'log...\n0.657407\n0.740741\n0.719626\n0.672897\n0.728972\n0.703929\n0.032729\n100\n\n\n215\n0.002077\n0.000097\n0.001317\n0.000395\nbalanced\nlog_loss\n10\nlog2\nrandom\n{'class_weight': 'balanced', 'criterion': 'log...\n0.657407\n0.666667\n0.691589\n0.663551\n0.663551\n0.668553\n0.011903\n179\n\n\n\n\n216 rows × 18 columns\n\n\n\nWe can directly call the best parameters:\n\n\nCode\n# decision tree with best parameters\nclf = DecisionTreeClassifier(**tree_clf_hyper.best_params_).fit(X_train, y_train)\n\n# prediction\ny_pred = clf.predict(X_test)\n\n# results (will set average to weighted for multiclass vs binary)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Test Accuracy: {accuracy}')\nprint(f'Test Precision: {precision}')\nprint(f'Test Recall: {recall}')\nprint(f'Test F1: {f1}')\n\n\nTest Accuracy: 0.7705627705627706\nTest Precision: 0.6956521739130435\nTest Recall: 0.6\nTest F1: 0.6442953020134228"
  },
  {
    "objectID": "cluster_analysis.html#import-libraries-and-make-data",
    "href": "cluster_analysis.html#import-libraries-and-make-data",
    "title": "Cluster Analysis",
    "section": "Import Libraries and Make Data",
    "text": "Import Libraries and Make Data\n\n\nCode\n# libraries\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# data\nx, y = make_blobs(n_samples=100,\n                  centers=4, n_features=2,\n                  cluster_std=[1,1.5,2, 2],\n                  random_state=7)\n                  \n# make blobs\ndf_blobs = pd.DataFrame({\n    'x1': x[:,0],\n    'x2':x[:,1],\n    'y':y\n\n})\n\ndf_blobs.head()\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-3.384261\n5.221740\n1\n\n\n1\n-1.836238\n-7.735384\n3\n\n\n2\n-7.456176\n6.198874\n0\n\n\n3\n-1.785043\n1.609749\n1\n\n\n4\n-10.124910\n6.133805\n0"
  },
  {
    "objectID": "cluster_analysis.html#d-plot-function",
    "href": "cluster_analysis.html#d-plot-function",
    "title": "Cluster Analysis",
    "section": "2-D Plot Function",
    "text": "2-D Plot Function\n\n\nCode\ndef plot_2d_clusters(x, y, ax):\n    y_uniques = pd.Series(y).unique()\n\n    for cl in y_uniques:\n        x[y==cl].plot(\n            title=f'{len(y_uniques)} Clusters',\n            kind='scatter',\n            x='x1',\n            y='x2',\n            marker = f'${cl}$',\n            ax = ax\n        )"
  },
  {
    "objectID": "cluster_analysis.html#first-image",
    "href": "cluster_analysis.html#first-image",
    "title": "Cluster Analysis",
    "section": "First Image",
    "text": "First Image\n\n\nCode\nfig, ax = plt.subplots(1,1, figsize=(15,10))\nx, y = df_blobs[['x1','x2']], df_blobs['y']\nplot_2d_clusters(x,y,ax)"
  },
  {
    "objectID": "cluster_analysis.html#applying-clustering",
    "href": "cluster_analysis.html#applying-clustering",
    "title": "Cluster Analysis",
    "section": "Applying Clustering",
    "text": "Applying Clustering\n\n\nCode\nkmeans = KMeans(n_clusters=100, random_state=7)\ny_pred = kmeans.fit_predict(x)\n\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n\n\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(20,12))\nplot_2d_clusters(x,y,axs[0])\nplot_2d_clusters(x,y_pred,axs[1])\n\naxs[0].set_title(f'Actual {axs[0].get_title()}')\naxs[1].set_title(f'Kmeans {axs[1].get_title()}')\n\n\nText(0.5, 1.0, 'Kmeans 100 Clusters')"
  },
  {
    "objectID": "cluster_analysis.html#test-on-real-data",
    "href": "cluster_analysis.html#test-on-real-data",
    "title": "Cluster Analysis",
    "section": "Test on Real Data",
    "text": "Test on Real Data\n\n\nCode\ndf = pd.read_csv('data/uber_clean.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nDate/Time\nLat\nLon\nBase\nDate\n\n\n\n\n0\n2014-07-01 0:03\n40.7586\n-73.9706\nB02512\nTuesday\n\n\n1\n2014-07-01 0:05\n40.7605\n-73.9994\nB02512\nTuesday\n\n\n2\n2014-07-01 0:06\n40.7320\n-73.9999\nB02512\nTuesday\n\n\n3\n2014-07-01 0:09\n40.7635\n-73.9793\nB02512\nTuesday\n\n\n4\n2014-07-01 0:20\n40.7204\n-74.0047\nB02512\nTuesday\n\n\n\n\n\n\n\n\n\nCode\nx = df[[\"Lat\", \"Lon\"]] # features\nx.head()\n\n\n\n\n\n\n\n\n\nLat\nLon\n\n\n\n\n0\n40.7586\n-73.9706\n\n\n1\n40.7605\n-73.9994\n\n\n2\n40.7320\n-73.9999\n\n\n3\n40.7635\n-73.9793\n\n\n4\n40.7204\n-74.0047\n\n\n\n\n\n\n\n\n\nCode\nmodel = KMeans(n_clusters = 3)\ny_kmeans = model.fit_predict(x)\ndf['y'] = y_kmeans  # store it in an actual frame\ndf.head()\nplt.scatter(df['Lon'], df['Lat'], c=df['y']) # colorized based on y-values (clusters based on lat long k-means)\n\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "cluster_analysis.html#inertia",
    "href": "cluster_analysis.html#inertia",
    "title": "Cluster Analysis",
    "section": "Inertia",
    "text": "Inertia\n\n\nCode\n# model inertia: measures how well a data set was clustered by k-means\n\n# meaure the distance between each data point and its centroid, square this distance, and sume up\n# these squares across one cluster\n\n# a good model is one with a low inertia value and low number of cluster (K)\n\nmodel.inertia_\n\n\n1957.720652418312"
  },
  {
    "objectID": "cluster_analysis.html#elbow-method-1",
    "href": "cluster_analysis.html#elbow-method-1",
    "title": "Cluster Analysis",
    "section": "Elbow Method",
    "text": "Elbow Method\n\n\nCode\nwcss = [] #  within-cluster sum of squares\nfor i in range(1,11):\n    model = KMeans(n_clusters = i)\n    y_kmeans = model.fit_predict(x)\n    wcss.append(model.inertia_)  # adding accuracy to our model\n    \nplt.plot(range(1,11), wcss)\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.title('Elbow Method')\nplt.show()\n\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\carlj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "cluster_analysis.html#visual-data-on-a-map",
    "href": "cluster_analysis.html#visual-data-on-a-map",
    "title": "Cluster Analysis",
    "section": "Visual Data on a Map",
    "text": "Visual Data on a Map\n\n\nCode\n# import folium\nimport folium\n\n\n\n\nCode\n# visaulize data in actual map\n\ndf = df[:2000]  # instead of 40,000\n\nclusters1  = df[['Lat', \"Lon\"]][df['y'] == 0].values.tolist()\nclusters2  = df[['Lat', \"Lon\"]][df['y'] == 1].values.tolist()\nclusters3  = df[['Lat', \"Lon\"]][df['y'] == 2].values.tolist()\n\n# map\ncity_map = folium.Map(location= [40.7128, -74.0060], zoom_start = 10, titles = \"openstreetmap\")\n\nfor i in clusters1:\n    folium.CircleMarker(i, radius =2, color = 'blue', fill_color = 'lightblue').add_to(city_map)\n\nfor i in clusters2:\n    folium.CircleMarker(i, radius =2, color = 'red', fill_color = 'lightred').add_to(city_map)\n\nfor i in clusters3:\n    folium.CircleMarker(i, radius =2, color = 'green', fill_color = 'lightgreen').add_to(city_map)\n    \ncity_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "pca.html#import-libraries-and-data",
    "href": "pca.html#import-libraries-and-data",
    "title": "Principal Component Analysis (PCA)",
    "section": "Import Libraries and Data",
    "text": "Import Libraries and Data\n\n\nCode\n# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n# from sklearn.preprocessing import StandardScaler (shown how to use in cell below)\n\n\n\n\nCode\n# Load the homework data set\n\n# Rename the columns based on their features.\n\ncolumns = ['class','alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n    'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n    'proanthocyanins', 'color_intensity', 'hue',\n    'dilution_of_wines', 'proline']\n\ndf = pd.read_csv('data/wine.csv', names=columns, header=0)\ndf.head()\n\n\n\n\n\n\n\n\n\nclass\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\ndilution_of_wines\nproline\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735"
  },
  {
    "objectID": "pca.html#scale-data",
    "href": "pca.html#scale-data",
    "title": "Principal Component Analysis (PCA)",
    "section": "Scale Data",
    "text": "Scale Data\nWith scale\n\n\nCode\n# Utilizing the standard scaler method to get the values converted into integers.\nX = df.iloc[:, 1:].values\nX_normal = scale(X)\n\n\nWith StandardScaler\n\n\nCode\nscaler = StandardScaler()\nX_normal = scaler.fit_transform(X)"
  },
  {
    "objectID": "pca.html#implement-pca",
    "href": "pca.html#implement-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "Implement PCA",
    "text": "Implement PCA\n\nQuestion 1- Using Principal Component Analysis or PCA in short to reduce the dimensionality of the data in order to optimize the result of the clustering (5 Points)\n\n\nCode\n# implement pca with the normalized features\npca = PCA()\nprincipalComponents = pca.fit_transform(X_normal)\n\n\n\n\nQuestion 2 - Create a dataframe featuring the Principal components that you acquired through PCA and show the output (5 Points)\n\n\nCode\n# create the dataframe from the principal components\nprincipalDf = pd.DataFrame(data = principalComponents)\nprincipalDf.columns = [f'principal component {col + 1}' for col in range(principalDf.shape[1])]\n\n# display the head of data\nprincipalDf.head()\n\n\n\n\n\n\n\n\n\nprincipal component 1\nprincipal component 2\nprincipal component 3\nprincipal component 4\nprincipal component 5\nprincipal component 6\nprincipal component 7\nprincipal component 8\nprincipal component 9\nprincipal component 10\nprincipal component 11\nprincipal component 12\nprincipal component 13\n\n\n\n\n0\n3.316751\n-1.443463\n-0.165739\n-0.215631\n0.693043\n-0.223880\n0.596427\n0.065139\n0.641443\n1.020956\n-0.451563\n0.540810\n-0.066239\n\n\n1\n2.209465\n0.333393\n-2.026457\n-0.291358\n-0.257655\n-0.927120\n0.053776\n1.024416\n-0.308847\n0.159701\n-0.142657\n0.388238\n0.003637\n\n\n2\n2.516740\n-1.031151\n0.982819\n0.724902\n-0.251033\n0.549276\n0.424205\n-0.344216\n-1.177834\n0.113361\n-0.286673\n0.000584\n0.021717\n\n\n3\n3.757066\n-2.756372\n-0.176192\n0.567983\n-0.311842\n0.114431\n-0.383337\n0.643593\n0.052544\n0.239413\n0.759584\n-0.242020\n-0.369484\n\n\n4\n1.008908\n-0.869831\n2.026688\n-0.409766\n0.298458\n-0.406520\n0.444074\n0.416700\n0.326819\n-0.078366\n-0.525945\n-0.216664\n-0.079364\n\n\n\n\n\n\n\n\n\nQuestion 3 - Compute the amount of variance that each PCA explains. Dispay the output (10 Points)\n\n\nCode\n# compute explained variance each PCA explains\nexplained_variance = pca.explained_variance_ratio_\n\n# numerical array of explained variance\nexplained_variance\n\n# dataframe of explained variance\nexplained_variance_df = pd.DataFrame({\n    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance))],\n    'Explained Variance': explained_variance\n})\n\nexplained_variance_df.round(4)\n\n\n\n\n\n\n\n\n\nPrincipal Component\nExplained Variance\n\n\n\n\n0\nPC1\n0.3620\n\n\n1\nPC2\n0.1921\n\n\n2\nPC3\n0.1112\n\n\n3\nPC4\n0.0707\n\n\n4\nPC5\n0.0656\n\n\n5\nPC6\n0.0494\n\n\n6\nPC7\n0.0424\n\n\n7\nPC8\n0.0268\n\n\n8\nPC9\n0.0222\n\n\n9\nPC10\n0.0193\n\n\n10\nPC11\n0.0174\n\n\n11\nPC12\n0.0130\n\n\n12\nPC13\n0.0080\n\n\n\n\n\n\n\n\n\nQuestion 4 - Calculate the cummulative variances to 4 decimals places. Display the output (10 points)\n\n\nCode\n# compute cumulative variance through each PCA (to 4 decimal places)\ncumulative_variance = np.cumsum(explained_variance.round(4))\n\n# numerical array of cumulative variance\ncumulative_variance\n\n# dataframe of cumulative variance\ncumulative_variance_df = pd.DataFrame({\n    'Principal Component': [f'PC{i+1}' for i in range(len(cumulative_variance))],\n    'Cumulative Variance': cumulative_variance\n})\n\ncumulative_variance_df\n\n\n\n\n\n\n\n\n\nPrincipal Component\nCumulative Variance\n\n\n\n\n0\nPC1\n0.3620\n\n\n1\nPC2\n0.5541\n\n\n2\nPC3\n0.6653\n\n\n3\nPC4\n0.7360\n\n\n4\nPC5\n0.8016\n\n\n5\nPC6\n0.8510\n\n\n6\nPC7\n0.8934\n\n\n7\nPC8\n0.9202\n\n\n8\nPC9\n0.9424\n\n\n9\nPC10\n0.9617\n\n\n10\nPC11\n0.9791\n\n\n11\nPC12\n0.9921\n\n\n12\nPC13\n1.0001\n\n\n\n\n\n\n\n\n\nQuestion 5 - Compute the Variance plot for PCA components obtained and comment on the plot (10 points)\n\n\nCode\n# plot for individual explained variance\nplt.figure()\nplt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center', color = 'blue', label = 'Explained Variance')\nplt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, color = 'red', label = 'Cumulative Variance')\nplt.legend()\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.xticks(range(1, len(explained_variance) + 1))\nplt.title('Individual Variance Explained by Each Principal Component')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn the chart above, we have displayed the individual explained variance and the cumulative variance of each principal component. We can see that Principal Component 1 individually explains the most of the variance in the model right below 40%, and then the amount of variance the other components explain start to taper off. The cumulative explained variance reaches about 80% at Principal Component 5.\nDepending on our threshold (either explained variance or number of components), we have some information to choose which components we would like to include.\nIf we wanted to keep our model to 3 principal components, we would get just about 60% explained variance.\nHowever, by including up to 5 or 6 principal components, we could increase our explained variance into the 80%s.\n\n\nQuestion 6 - As our results are suggesting to use first 3 principal components for further computation, extract the three features from the PCA_dataset into PCA1, PCA2, PCA3 (10 points)\n\n\nCode\n# extract the first three principal components into a final dataframe\nfinal_df = principalDf.iloc[:, :3]\n\nfinal_df.head()\n\nfinal_df.shape\n\n\n(178, 3)\n\n\n\n\nQuestion 7 - Create a dataframe for further clusering algorithms with PCA1, PCA2 and PCA3 as column headings. Display your results (10 points)\n\n\nCode\n# join class column into final dtaframe\nfinal_df = pd.concat([final_df, df['class']], axis=1)\n\n# change column headings\nfinal_df.columns = ['PCA1', 'PCA2', 'PCA3', 'Class']\n\n# double check shape\nfinal_df.shape\n\n# double check nulls\nfinal_df.isnull().sum()\n\n# display the head of the results\nfinal_df.head()\n\n\n\n\n\n\n\n\n\nPCA1\nPCA2\nPCA3\nClass\n\n\n\n\n0\n3.316751\n-1.443463\n-0.165739\n1\n\n\n1\n2.209465\n0.333393\n-2.026457\n1\n\n\n2\n2.516740\n-1.031151\n0.982819\n1\n\n\n3\n3.757066\n-2.756372\n-0.176192\n1\n\n\n4\n1.008908\n-0.869831\n2.026688\n1\n\n\n\n\n\n\n\n\n\nQuestion 8 - As done in class, go ahead and visualize the results of the 3D PCA. Properly label the x, y, and z- axis. Comment and summarize your results after the plot. (20 points)\n\n\nCode\n# visualize the results\nfig = plt.figure(figsize = (12,10))\nax = fig.add_subplot(1,1,1,  projection='3d')\nax.set_xlabel('PCA1', fontsize = 15)\nax.set_ylabel('PCA2', fontsize = 15)\nax.set_zlabel('PCA3', fontsize = 15)\nax.set_title('3 Component PCA', fontsize = 20)\n\ntargetsName = ['1', '2', '3']\n\ntargets = [1, 2, 3]\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets, colors):\n    indicesToKeep = final_df['Class'] == target\n    ax.scatter(final_df.loc[indicesToKeep, 'PCA1'],\n               final_df.loc[indicesToKeep, 'PCA2'],\n               final_df.loc[indicesToKeep, 'PCA3'],\n               c = color,\n               s = 50)\nax.legend(targetsName)\nax.grid()\n\n\n\n\n\n\n\n\n\nIn the plot above, we created a 3-dimensional plot to display the result of our principal component analysis. Our three target classes of wine were given different colors:\n\n1: Red\n2: Green\n3: Blue\n\nAs seen by the coloring, the PCA does a decent job of creating clusters between the 3 classes of wine. For the most part, each class of wine is within their own cluster. However, there is some minor overlap. There is overlap between Class 1 and Class 2, and then again between Class 2 and Class 3. However, it appears there is no overlap between the clusters of Class 1 and Class 3."
  },
  {
    "objectID": "cluster_analysis.html#visualize-data-on-a-map",
    "href": "cluster_analysis.html#visualize-data-on-a-map",
    "title": "Cluster Analysis",
    "section": "Visualize Data on a Map",
    "text": "Visualize Data on a Map\n\n\nCode\n# import folium\nimport folium\n\n\n\n\nCode\n# visaulize data in actual map\n\ndf = df[:2000]  # instead of 40,000\n\nclusters1  = df[['Lat', \"Lon\"]][df['y'] == 0].values.tolist()\nclusters2  = df[['Lat', \"Lon\"]][df['y'] == 1].values.tolist()\nclusters3  = df[['Lat', \"Lon\"]][df['y'] == 2].values.tolist()\n\n# map\ncity_map = folium.Map(location= [40.7128, -74.0060], zoom_start = 10, titles = \"openstreetmap\")\n\nfor i in clusters1:\n    folium.CircleMarker(i, radius =2, color = 'blue', fill_color = 'lightblue').add_to(city_map)\n\nfor i in clusters2:\n    folium.CircleMarker(i, radius =2, color = 'red', fill_color = 'lightred').add_to(city_map)\n\nfor i in clusters3:\n    folium.CircleMarker(i, radius =2, color = 'green', fill_color = 'lightgreen').add_to(city_map)\n    \ncity_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  }
]
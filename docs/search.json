[
  {
    "objectID": "web_scraping.html",
    "href": "web_scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "This page has information about web scraping."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This website contains knowledge, techniques, and examples on the topic of data mining."
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "This page has information about data visualization, containing types of plots, use cases, and how to implement using various python libraries.\nLecture 5"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nCode1 + 1\n\n[1] 2"
  },
  {
    "objectID": "frequent_patterns.html",
    "href": "frequent_patterns.html",
    "title": "Frequent Patterns",
    "section": "",
    "text": "We’re trying to find frequent itemsets given a list of sets. Essentially, finding the subsets within these sets that are commonly together.\nTwo prime examples of this are items purchased in transactions and movies watched in a library.\n\n\n\n\nSupport quantifies how often an itemset appears in a dataset (proportion).\nLet \\(I\\) be an itemset, then:\n\\(Support(I) = \\frac{\\text{number of sets containing } I}{\\text{total number of sets}}\\)\nSuppose we have 100 customer’s movie watchlists, and we’re interested in making association rules surrounding movie \\(M\\).\nWe know that movie (or movie set) \\(M\\) is in 10 of the watchlists.\n\\(Support(M) = \\frac{10}{100}\\)\nAdditionally, we could have support for an association rule (\\(Support(A \\rightarrow C)\\)), where \\(A\\) is the antecendent and \\(C\\) is the consequent (both are itemsets), which would look like:\n\\(Support(A \\rightarrow C) = \\frac{\\text{number of sets containing } A \\text{ and } C}{\\text{total number of sets}}\\)\n\n\n\nConfidence quantifies the likelihood an itemset’s consequent occurs given its antecent (i.e. the probability a consequent occurs given its antecent).\nLet \\(A\\) be an itemset’s antecent and \\(C\\) be an itemset’s consequent, then:\n\\(Confidence(A \\rightarrow C) = \\frac{\\text{proportion of sets containing} A \\text{ and } C }{\\text{proportion of sets containing} A}\\)\nNote that the concept of confidence is roughly derived with the help of the concept of conditional probability.\n\n\\(Confidence(A \\rightarrow C) = P(C|A) = \\frac{P(C, A)}{P(A)}\\)\n\nA key difference is that the numerator is is not a true intersection of events, but it contains the every element in itemsets \\(A\\) and \\(C\\). It’s more clear to use the respective definitions of support here.\n\\(Confidence(A \\rightarrow C) = \\frac{Support(A \\cup C)}{Support(A)}\\)\nSuppose we want to recommend a movie (or movie set), \\(M_2\\), and we’re creating rules based on those who have watched the movie (or movie set) \\(M_2\\).\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{\\text{proportion of watchlists containing} M_1 \\text{ and } M_2} {\\text{proportion of watchlists containing} M_1}\\)\n\\(= \\frac{Support(M_1 \\cup M_2)}{Support(M_1)}\\)\n\n\n\nLift assesses the performance of an association rule by quantifying an improvement (or degradation) from the initial prediction, where the initial prediction would be the support of the antecent.\n\\(Lift(C \\rightarrow A) = \\frac{Confidence(A \\rightarrow C)}{Support(C)}\\)\n\\(= \\frac{\\frac{Support(A \\cup C)}{Support(A)}}{Support(C)}\\)\n\\(= \\frac{Support(A \\cup C)}{Support(A)Support(C)}\\)\nSuppose we have a rule suggesting we recommend a movie (or movie set), \\(M_2\\), to those have seen movie(s) \\(M_1\\) (i.e. \\(M_1 \\rightarrow M_2\\)).\nWe gather the following measurements:\n\n\\(Support(M_1) = P(M_1) = 0.4\\)\n\\(Support(M_2) = P(M_2) = 0.1\\)\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{Support(M_1 \\cup M_2)}{Support(M_1)} = 0.175\\)\n\\(Lift(M_1 \\rightarrow M_2) = \\frac{Confidence(M_1 \\rightarrow M_2)}{Support(M_2)} = 1.75\\)\n\n\n\n\nBecause \\(lift((M_1 \\rightarrow M_2)) &gt; 1\\), this rule provides evidence that the suggesting \\(M_2\\) to those who have have seen \\(M_1\\) is better than just suggesting \\(M_2\\).\nIn general, this yields the result that if \\(lift &gt; 1\\), the association rule improves our initial prediction.\n\n\n\nMaximal Itemset: itemset in which none of its supersets are frequent.\n\n\n\nClosed Itemset: itemset in which none of its immediate supersets have the same support count as the itemset, itself.\n\n\n\nk-itemset: itemset which contains k items.\n\nWhen finding frequencies, can refer to each sized frequency set as n-frequent, where n is the number of items in an itemset (or subset).\n\n\n\n\nApriori Property: all non-empty subsets of frequent itemsets must be frequent.\n\nGiven a frequent itemset, all non-empty subsets of the frequent itemset are frequent as well.\n\n\n\n\nAntimonotonicity: If a set cannot pass a test, all its supersets will fail the test as well.\nRoughly the contrapositive of the Apriori Property for association rules.\n\nIf an itemset is infrequent, all its supersets will be infrequent as well.\n\n\n\n\n\n\\(2^n - 1\\), where \\(n\\) is the total number of items in the dataset. This is actually problematic due to the factorial growth per element.\n\nFor example, take a frequent itemset with just 100 elements. The total number of frequent itemsets that it contains is:\n\\(\\sum\\limits_{n = 1}^{1000}{100 \\choose n} = 2^{100} - 1 \\approx 1.27 x 10^{30}\\)"
  },
  {
    "objectID": "frequent_patterns.html#the-basics",
    "href": "frequent_patterns.html#the-basics",
    "title": "Frequent Patterns",
    "section": "",
    "text": "Support quantifies how often an itemset appears in a dataset (proportion).\nLet \\(I\\) be an itemset, then:\n\\(Support(I) = \\frac{\\text{number of sets containing } I}{\\text{total number of sets}}\\)\nSuppose we have 100 customer’s movie watchlists, and we’re interested in making association rules surrounding movie \\(M\\).\nWe know that movie (or movie set) \\(M\\) is in 10 of the watchlists.\n\\(Support(M) = \\frac{10}{100}\\)\nAdditionally, we could have support for an association rule (\\(Support(A \\rightarrow C)\\)), where \\(A\\) is the antecendent and \\(C\\) is the consequent (both are itemsets), which would look like:\n\\(Support(A \\rightarrow C) = \\frac{\\text{number of sets containing } A \\text{ and } C}{\\text{total number of sets}}\\)\n\n\n\nConfidence quantifies the likelihood an itemset’s consequent occurs given its antecent (i.e. the probability a consequent occurs given its antecent).\nLet \\(A\\) be an itemset’s antecent and \\(C\\) be an itemset’s consequent, then:\n\\(Confidence(A \\rightarrow C) = \\frac{\\text{proportion of sets containing} A \\text{ and } C }{\\text{proportion of sets containing} A}\\)\nNote that the concept of confidence is roughly derived with the help of the concept of conditional probability.\n\n\\(Confidence(A \\rightarrow C) = P(C|A) = \\frac{P(C, A)}{P(A)}\\)\n\nA key difference is that the numerator is is not a true intersection of events, but it contains the every element in itemsets \\(A\\) and \\(C\\). It’s more clear to use the respective definitions of support here.\n\\(Confidence(A \\rightarrow C) = \\frac{Support(A \\cup C)}{Support(A)}\\)\nSuppose we want to recommend a movie (or movie set), \\(M_2\\), and we’re creating rules based on those who have watched the movie (or movie set) \\(M_2\\).\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{\\text{proportion of watchlists containing} M_1 \\text{ and } M_2} {\\text{proportion of watchlists containing} M_1}\\)\n\\(= \\frac{Support(M_1 \\cup M_2)}{Support(M_1)}\\)\n\n\n\nLift assesses the performance of an association rule by quantifying an improvement (or degradation) from the initial prediction, where the initial prediction would be the support of the antecent.\n\\(Lift(C \\rightarrow A) = \\frac{Confidence(A \\rightarrow C)}{Support(C)}\\)\n\\(= \\frac{\\frac{Support(A \\cup C)}{Support(A)}}{Support(C)}\\)\n\\(= \\frac{Support(A \\cup C)}{Support(A)Support(C)}\\)\nSuppose we have a rule suggesting we recommend a movie (or movie set), \\(M_2\\), to those have seen movie(s) \\(M_1\\) (i.e. \\(M_1 \\rightarrow M_2\\)).\nWe gather the following measurements:\n\n\\(Support(M_1) = P(M_1) = 0.4\\)\n\\(Support(M_2) = P(M_2) = 0.1\\)\n\\(Confidence(M_1 \\rightarrow M_2) = \\frac{Support(M_1 \\cup M_2)}{Support(M_1)} = 0.175\\)\n\\(Lift(M_1 \\rightarrow M_2) = \\frac{Confidence(M_1 \\rightarrow M_2)}{Support(M_2)} = 1.75\\)\n\n\n\n\nBecause \\(lift((M_1 \\rightarrow M_2)) &gt; 1\\), this rule provides evidence that the suggesting \\(M_2\\) to those who have have seen \\(M_1\\) is better than just suggesting \\(M_2\\).\nIn general, this yields the result that if \\(lift &gt; 1\\), the association rule improves our initial prediction.\n\n\n\nMaximal Itemset: itemset in which none of its supersets are frequent.\n\n\n\nClosed Itemset: itemset in which none of its immediate supersets have the same support count as the itemset, itself.\n\n\n\nk-itemset: itemset which contains k items.\n\nWhen finding frequencies, can refer to each sized frequency set as n-frequent, where n is the number of items in an itemset (or subset).\n\n\n\n\nApriori Property: all non-empty subsets of frequent itemsets must be frequent.\n\nGiven a frequent itemset, all non-empty subsets of the frequent itemset are frequent as well.\n\n\n\n\nAntimonotonicity: If a set cannot pass a test, all its supersets will fail the test as well.\nRoughly the contrapositive of the Apriori Property for association rules.\n\nIf an itemset is infrequent, all its supersets will be infrequent as well.\n\n\n\n\n\n\\(2^n - 1\\), where \\(n\\) is the total number of items in the dataset. This is actually problematic due to the factorial growth per element.\n\nFor example, take a frequent itemset with just 100 elements. The total number of frequent itemsets that it contains is:\n\\(\\sum\\limits_{n = 1}^{1000}{100 \\choose n} = 2^{100} - 1 \\approx 1.27 x 10^{30}\\)"
  },
  {
    "objectID": "terminology.html",
    "href": "terminology.html",
    "title": "Terminology",
    "section": "",
    "text": "Data Mining is the process of discovering interesting patterns, models, and other kinds of knowledge from massive amounts of data.\nInterestingness is an either subjective or objective measure to guide the discovery process in data mining in which knowledge from data is judged on being novel, useful, and easily understood by humans."
  },
  {
    "objectID": "terminology.html#data-preparation",
    "href": "terminology.html#data-preparation",
    "title": "Terminology",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nCleaning\nRemoving noisy and inconsistent data.\nData in the real world is messy, and comes with potential for incorrectness whether that comes from instrument failures, human error, or transmission issues. Some errors we’ll find are:\n\nIncomplete Data: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data.\n\nignore the data\ndelete the data (if not too much data loss occurs)\nfill in data manually\nfill in data automatically with methods like averages or modes (fill all, backwards fill, forwards fill, etc.)\npossible smarter technique is to use an inference-based approach (Bayesian)\n\nNoisy: containing noise, errors, or outliers.\n\nRegression: fit data into regression functions\nClustering: detect and remove outliers\nSemi-supervised: detect suspicious values and check via human\n\nInconsistent: containing discrepancies in codes or names.\nIntentional: disguising missing data (i.e. dates either being the first or the last of a given month, or every birthday is January 1).\n\n\n\nIntegration\nMultiple data sources may be combined.\n\n\nTransformations\nData is transformed and consolidated into forms appropriate for mining.\nSome common transformation techniques are:\n\nsmoothing: remove noise from data\naggregation: summarization (i.e. daily \\(\\rightarrow\\) monthly)\ngeneralizatoin: concept hierarchy climbing (i.e. street \\(\\rightarrow\\) city \\(\\rightarrow\\) state)\nnormalization: scale to fall within a range\nattribute/feature construction\nDiscretization: process of converting continuous type data into discrete type data, which reduces the difficulty of data evaluation, management, and mining.\n\n\n\nSelection\nData relevant to the analysis task are retrieved.\n\n\nData Mining\nProcess where intelligent methods are applied to extract patterns or construct models.\n\n\nPattern/Model Evaluation\nIdentify patterns or models representing knowledge based on interestingness measures.\n\n\nKnowledge Presentation\nUsing visualization and knowledge representation techniques to present mined knowledge."
  },
  {
    "objectID": "terminology.html#data-reduction",
    "href": "terminology.html#data-reduction",
    "title": "Terminology",
    "section": "Data Reduction",
    "text": "Data Reduction\nData reduction is used to to reduce the size of the data (less storage, faster processing, etc.). The data is smaller in volume but can produce similary analytical results. This can be relevant for multiple steps in the data mining pipeline. Some method of data reduction are:\n\nregression and log-linear models\nhistograms, clustering, sampling\ndata cube aggregation\ndata compression\nSampling\n\n\nParametric Data Reduction: Regression Analysis\nRegression analysis is a collective name for techniques involving modeling and analysis of numerical data consisting of values of a dependent variable (response variable) and one or more independent variables (predictors). Some common regression analysis techniques are:\n\nlinear regression\nmultiple linear regression\nnonlinear regression\nlog-linear regression\n\n\n\nNon-Parametric Data Reduction:\n\nData Cube Aggregation\nHistogram Analysis\nClustering\n\n\n\nDimensionality Reduction\nThe curse of dimensionality: data becomes increasingly sparse, density and distance between points becomes less meaningful (detrimental for clustering and outlier analysis), and the possible subspaces will grow exponentially.\nDimensionality reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables.\nNot only do dimensionality reduction techniques help to avoid the curse of dimensionality, but also eliminate irrelevant features and reduce noise. Overall, this reduces the time and space complexity required for data mining and allows for easier visualization.\nMethodologies:\n\nfeature selection: find a subset o the original variables.\nfeature extraction: transform the data in the high-dimensional space to a space of fewer dimensions.\n\nA common technique is Principal Component Analysis (PCA). It’s a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The original data are projected onto a much smaller space, resulting in dimensionality reduction."
  },
  {
    "objectID": "terminology.html#record-data",
    "href": "terminology.html#record-data",
    "title": "Terminology",
    "section": "Record Data",
    "text": "Record Data\n\nRelational records or tables (highly structured)\nData matrix, numerical matrix, crosstabs\nTransaction data\nDocument data: term-frequency vector from text documents"
  },
  {
    "objectID": "terminology.html#graphs-networks",
    "href": "terminology.html#graphs-networks",
    "title": "Terminology",
    "section": "Graphs & Networks",
    "text": "Graphs & Networks\n\nTransportation network\nWorld Wide Web\nMolecular Structures\nSocial Information Networks"
  },
  {
    "objectID": "terminology.html#ordered-data",
    "href": "terminology.html#ordered-data",
    "title": "Terminology",
    "section": "Ordered Data",
    "text": "Ordered Data\n\nVideo Data (sequences of images)\nTemporal Data (time series)\nSequential Data (i.e. Transaction Sequences)\nGenetic Sequence Data"
  },
  {
    "objectID": "terminology.html#spatial-image-multimedia-data",
    "href": "terminology.html#spatial-image-multimedia-data",
    "title": "Terminology",
    "section": "Spatial, Image, & Multimedia Data",
    "text": "Spatial, Image, & Multimedia Data\n\nSpatial Data\nImage Data\nVideo Data"
  },
  {
    "objectID": "terminology.html#measures-of-central-tendency",
    "href": "terminology.html#measures-of-central-tendency",
    "title": "Terminology",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\n\nmean: also known as the average, and has some variations:\n\nstandard mean: \\(\\bar{x} = \\frac{1}{n} \\sum\\limits_{i=1}^{n}x_i\\)\nweighted mean: \\(\\bar{x} = \\frac{\\sum\\limits_{i=1}^{n}x_i w_i}{\\sum\\limits_{i=1}^{n}w_i}\\)\ntrimmed mean: removing outliers (extreme values) before calculating the mean\n\nmedian: middle-most value of ordered set (exact middle if odd numbered set, average of the middle if even numbered set)\nmode: most frequent value(s) in a set\nmidrange: \\(\\frac{\\text{minimum + maximum}}{2}\\)\n\nThey each have their importance.\nThe mean is one of the more common measures to describe a dataset. For a dataset which is symmetric, it is the center of the distribution. We use a weighted mean when each value has a relevant associated significance, importance, or occurence frequency. The trimmed mean is useful because even a small number of extreme occurences can corrupt the mean. We can remove small portions of the top and bottom of an ordered dataset, but should avoid removing too much as that can lead to unwanted information loss.\nExamples of when the standard form of mean is used is when comparing ages for a dataset or grades on an exam. A prime example for weighted means is calculating grade point average (GPA), where a grade might be a decimal, but is weighted by the number of credits it is worth.\nFor skewed datasets, or if it’s desired to retain outliers, the median is a better measure of the center of the data.\nSince trimmed means and medians are both good to combat the effect of skew and outliers, a decent example for both of these could be when calculating metrics from a dataset containing salaries. Image we have values with a median around $100,000, but there are just a few values upwards of $500,000. This could significantly alter the standard form of average, but if we trim off these high (and/or low) outliers, and then take the trimmed mean we might see it closer to the median.\nThe mode is useful for both qualitative and quantitative attributes, and represents the highest frequency of an attribute. There can be multiple modes, in which case the data is multimodal (unimodal, bimodal, trimodal for 1, 2 and 3 modes). Or, if each value only occurs once, there is no mode! A good visual for representing mode can be a bar chart. Imagine you’re trying to find the day of the week you have the most customers, the day the highest frequency of customers come in, which day has the tallest bar on the bar chart?\nFor a perfectly symmetric curve, the mean, median, and mode are all at the center of the distribution.\nThe midrange is actually another type of mean that can make for a quick statistic in a dataset. However, for asymmetric data, or data with extreme outliers, it’s unreliable."
  },
  {
    "objectID": "terminology.html#measures-of-dispersion",
    "href": "terminology.html#measures-of-dispersion",
    "title": "Terminology",
    "section": "Measures of Dispersion",
    "text": "Measures of Dispersion\n\nRange: the difference between the maximum and minimum values\nQuartiles:\n\n\\(Q_1\\): \\(25^{th}\\) percentile of data (point at which 25% of the data lies to the left on the distribution)\n\\(Q_2\\): \\(50^{th}\\) percentile of data (also known as the median)\n\\(Q_3\\): \\(75^{th}\\) percentile of data\n\nVariance & Standard Deviation: measure of how much the values deviate from the mean, or simply put, how spread apart the data is. The Standard Deviation is the square root of the Variance. They can be calculated in the following manner:\n\nPopulation Variance: \\(Var(X) = \\frac{1}{N} \\sum\\limits_{i=1}^{N}(x_i - \\mu)^2\\)\nSample Variance: \\(Var(X) = \\frac{1}{n-1} \\sum\\limits_{i=1}^{n}(x_i - \\mu)^2\\)\nIt can also be shown: \\(Var(X) = E[X^2] - E[X]^2\\)\n\\(SD(X) = \\sqrt{Var(X)}\\)\n\nInterquartile Range: the range covered by the middle half of the data, calculated by \\(IQR = Q_3 - Q_1\\)\n\nRange, Quartiles, Variance, Standard Deviation, and the Interquartile Range are known as the five-number summary, and together make up a clearer picture of the data. A rule of thumb about outliers is that a suspected outlier is anything at least \\(1.5 x IQR\\) above \\(Q_3\\) or below \\(Q_1\\).\nOften times, it’s helpful to consider all of these metrics together to start forming hypotheses about data. A good way to visualize the five-number summary is with a box plot."
  },
  {
    "objectID": "terminology.html#similarity-and-dissimilarity",
    "href": "terminology.html#similarity-and-dissimilarity",
    "title": "Terminology",
    "section": "Similarity and Dissimilarity",
    "text": "Similarity and Dissimilarity\n\nDissimilarity Matrix: given a data matrix containing \\(n\\) objects with \\(p\\) attributes, a dissimilarity matrix stores a collection of proximities for all pairs of \\(n\\) objects (usually an \\(nxn\\) table)\n\n\\(d(i, j)\\) is the distance between objects \\(i\\) and \\(j\\)\n\\(d(i, i) = 0\\)\n\\(d(i, j) = d(j, i)\\)\n\nIf \\(d(i, j)\\) is a measure of dissimilarity, then a measure of similarity is \\(sim(i, j) = 1 - d(i, j)\\)\nDissimilarity for Nominal Attributes: \\(d(i, j) = \\frac{p-m}{p}\\); \\(p\\) is the total attributes and \\(m\\) are the matches (weights can be assigned to \\(m\\) as well. This also implies \\(sim(i, j) = 1 - d(i, j) = \\frac{m}{p}\\)\nProximity Measures for Binary Attributes: notably the Jaccard coefficient is used for this. If we let:\n\n\\(q\\) be the number of attributes that both objects have\n\\(r\\) the number of attributes that object \\(i\\) has but not object \\(j\\)\n\\(s\\) the number of attributes that object \\(j\\) has but not object \\(i\\)\n\\(t\\) the number of attributes that neither object has, then…\nJaccard coefficient \\(= sim(i, j) = \\frac{q}{q+r+s} = 1 - d(i, j)\\)\n\nDissimilarity of Numeric Data: Minkowski Distance (Manhattan, Euclidean, etc.). The Minkowski distance generalizes the Manhattan, Euclidean and higher order distance measurements:\n\nManhattan: \\(d(i, j) = \\sum\\limits_{n=1}^{p}|x_{in} - x_{jn}|\\)\nEuclidean: \\(d(i, j) = \\sqrt{\\sum\\limits_{n=1}^{p} (x_{in} - x_{jn})^2}\\)\nMinknowski Generalization: \\(d(i, j) = (\\sum\\limits_{n=1}^{p} |x_{in} - x_{jn}|)^{\\frac{1}{h}}\\)\n\nThere are a plethora of other variations as well… included dissimilarity functions for Ordinal and Mixed attributes, and even a measure of similarity between documetns which used frequency vectors to produce Cosine Similarities\n\nPerhaps the measure listed above most people are familiar with is the Euclidean Distance. This was likely learned in high school, and it’s still a very common method to measure distances between points.\nFor a single example, these methods can be useful in finding clusters. Suppose a dataset isn’t showing any particular linear pattern. What happens if we set a maximum distance between points to be considered a certain family? Clusters may form."
  },
  {
    "objectID": "terminology.html#data-warehouse",
    "href": "terminology.html#data-warehouse",
    "title": "Terminology",
    "section": "Data Warehouse",
    "text": "Data Warehouse\nA data warehouse is a decision support database maintained separately from an organization’s operational database.\nDatabase warehouses support information processing by providing a solid platform of consolidated, historical data for analysis.\n“It is a subject-oriented, integrated, time-variant, and nonvolatile collection of data that support management’s decision-making process.” - William H. Inmon\n\nSubject Oriented\n\nOrganized around major subjects\nFocus on modeling and analyzing data for decision making, not on daily operations or transaction processing.\nExcluding data that are not useful in decision support process provides a simple and concise view of a particular subject issue.\n\n\n\nIntegrated\n\nIntegrate multiple, heterogeneous data sources (relational databases, flat files, online transaction records)\nData cleaning and data integration techniques were applied\nEnsure consistency in naming, encoding, attribute measures, etc.\n\n\n\nTime-Variant\n\nSignificantly longer time span\n\noperational database: current data or recent data\ndata warehouse: historical perspective\n\nEvery key structure in a data warehouse\n\ncontains time information, explicitly or implicitly\nkey of operational data may not contain time information\n\n\n\n\nNon-Volatile\n\nA physically separate store of data transformed from operational environments\nNo operational update of data (no transaction processing, recovery, concurrency control)\nOnly two operations in data accessing\n\nInitial Loading of data\nAccess of data\n\n\n\n\nHow do Data Warehouses Differ from Operational DBMS\n\nOLTP (online transaction processing)\n\nmajor task of traditional relational DBMS (3NF expected)\nday-to-day operations\nclerk, IT professionals, etc.\nusually thousands of users\napplication oriented\ncurrent, up-to-date, flat relational isolated\nsmallish records accessed (GBs)\n\nOLAP (online analytical processing)\n\nmajor task of the data warehouse system\ndata analysis and decision making\nknowledge analysis\nusually hundreds of users\nsubject oriented\nhistorical, summarized, multidimensional, integrated, consolidated\nmillions of records accessed (TBs)\n\n\nHowever, we usually need them both!"
  },
  {
    "objectID": "terminology.html#data-cube",
    "href": "terminology.html#data-cube",
    "title": "Terminology",
    "section": "Data Cube",
    "text": "Data Cube\nA data cube allows for data to be modeled and viewed in multiple dimensions."
  },
  {
    "objectID": "terminology.html#conceptual-modeling",
    "href": "terminology.html#conceptual-modeling",
    "title": "Terminology",
    "section": "Conceptual Modeling",
    "text": "Conceptual Modeling\n\nStar Schema\nA fact table, a set of dimension tables.\n\n\nSnowflake Schema\nA fact table, a hierarchy of dimension tables.\n\n\nFact Constellations\nMultiple fact tables share dimension tables.\n\n\nOLAP Operations\nThe data cube allows for many operations, most notably:\n\ndrilling up (city \\(rightarrow\\) state)\ndrilling down (state \\(rightarrow\\) city)\nslice: project with one dimension\ndice: project with multiple dimensions (sub-cube)\npivot (rotate): visualization, 3D to 2D\n\n\n\nIceberg Cube\nAnalogy: Only a small portion may be above the water in a sparse cube.\nWe compute only the cuboid cells whose aggregate (i.e. count) is above a threshold (a minimum support).\nAvoid explosive growth of the cube."
  },
  {
    "objectID": "terminology.html#the-basics",
    "href": "terminology.html#the-basics",
    "title": "Terminology",
    "section": "The Basics",
    "text": "The Basics\n\nPattern: A set of items, subsequences, or substructures that occur frequently together (or are strongly correlated) in a data set. Patterns represent intrinsic and important properties of datasets.\n\nPattern Discovery is uncovering these structures from massive data sets, and is the foundation for many essential data mining tasks, such as:\n\nassociation, correlation, and causality analysis\nmining sequential structural patterns\nclassification: discriminative pattern-based analysis\ncluster analysis: pattern-based subspace clustering\n\nAnd features broad applications, such as:\n\nmarket basket analysis, cross-marketing, catalog design, sale campaign analysis, web log analysis, biological sequence analysis\nmany types of data: spatiotemporal, multimedia, time-series, and steam data\n\nSee the Frequent Patterns tab for more detailed information."
  },
  {
    "objectID": "frequent_patterns.html#outline",
    "href": "frequent_patterns.html#outline",
    "title": "Frequent Patterns",
    "section": "Outline",
    "text": "Outline\nThe key steps of the Apriori Algorithm follow this computation process: - initial iteration: count number of occurrences of each item to form candidate 1-itemsets, \\(C_1\\) - initial minimum support check: compare \\(C_1\\) with the minimum support, removing any occurrence which does not pass the minimum support test, to form frequent 1-itemsets, \\(L_1\\) - future iterations: form candidate k-itemsets, \\(C_k\\) where \\(k &gt; 1\\) - join \\(L_{k-1}\\) with itself such that all permutations are formed with length of \\(k\\) - future iterations: form frequent k-itemsets, \\(L_k\\) where \\(k &gt; 1\\) - compare \\(C_k\\) with the minimum support, removing any occurrence which does not pass the minimum support test - repeat until maximum transaction length from dataset or no candidate or frequent itemsets can be generated\n\nNote that by the Apriori pincipal that all non-empty subsets of the frequent itemset are frequent as well, we can report the final \\((k+1)\\)-length frequent itemsets, as those contain the non-empty frequent itemsets."
  },
  {
    "objectID": "frequent_patterns.html#implementation",
    "href": "frequent_patterns.html#implementation",
    "title": "Frequent Patterns",
    "section": "Implementation",
    "text": "Implementation\n\nCustom Example\n\n\nCode\nfrom itertools import permutations\ndef custom_apriori(transaction_dict, min_sup = 0.5):\n    # get all unique items in transactions\n    freq_items = set()\n    for transaction in transaction_dict:\n        freq_items = freq_items.union(transaction_dict[transaction])\n        \n    # begin with frequent k=1 itemsets\n    # boolean initialized to true for meeting support, will exit when false\n    # initialize frequency dictionary to store results\n    meets_support = True\n    k = 1\n    frequency_dict = {}\n    \n    while meets_support:\n        # create list for each permutation, initializing support count at 0\n        prev_perms = []\n        candidate = []\n        for perm in permutations(freq_items, k):\n            # prevent duplicates since order doesn't matter\n            if set(perm) not in prev_perms:\n                candidate.append([set(perm), 0])\n                prev_perms.append(set(perm))\n\n        # loop through each permutation in candidate\n        # if the intersection of the permutation with a transaction is the permutation\n        # then that indicates it is an itemset within the transaction\n        for itemset in candidate:\n            for transaction in transaction_dict:\n                if itemset[0].intersection(transaction_dict[transaction]) == itemset[0]:\n                    itemset[1] += 1\n        # keep the permutations with support count at least at the minimum support\n        frequency = [itemset[0] for itemset in candidate if (itemset[1] / len(transaction_dict)) &gt;= min_sup]\n        # if the frequency list is empty, there are no more permutations which can meet the minimum support threshold\n        # we can exit it at that case due to antimonotonicity\n        if len(frequency) != 0:\n            # add to frequency dictionary for each round\n            frequency_dict[k] = frequency\n            # extract unique items from new frequency itemsets\n            freq_items = set()\n            for itemset in frequency:\n                freq_items = freq_items.union(itemset)\n            k += 1\n            \n        else:\n            meets_support = False\n            \n    return frequency_dict\n\n\nSuppose we were given the example of\n\n\n\nfrequent_patterns_img1\n\n\n\n\nCode\ntransaction_dict = {'t1': {'M', 'N', 'P', 'X', 'Z'},\n                    't2': {'M', 'A', 'P', 'X', 'O'},\n                    't3': {'D', 'N', 'P', 'X', 'Z'},\n                    't4': {'M', 'O', 'C', 'P', 'Z'},\n                    't5': {'M', 'K', 'I', 'Z', 'O'}}\n\n\nA quick aside on the number of datasets:\n\\(2^n - 1 = 2^{11} - 1 = 2047\\) possible frequent itemsets.\nThe results of the Custom Apriori Algorithm will produce:\n\n\nCode\ncustom_apriori(transaction_dict)\n\n\n{1: [{'O'}, {'Z'}, {'P'}, {'X'}, {'M'}],\n 2: [{'P', 'X'}, {'M', 'O'}, {'M', 'Z'}, {'P', 'Z'}, {'M', 'P'}]}\n\n\nNote that we’re not giving the interim support counts or the final support counts, how about we take a look at a widely used and accepted module.\n\n\nThe mlxtend Library\nImport the following from mlxtend:\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori as ml_apriori\nfrom mlxtend.frequent_patterns import association_rules as ml_association\n\n\nFor the TransactionEncoder, we’re going to want the transactions in a list of lists:\n\n\nCode\n# turn transactions into list of lists\ntransactions = [list(transaction_dict[transaction]) for transaction in  transaction_dict]\n\n\nEncode the transactions list:\n\n\nCode\n# put the transactions' lists in a boolean dataframe format\ncon = TransactionEncoder()\ncon_arr = con.fit(transactions).transform(transactions)\ndf_transformed = pd.DataFrame(con_arr, columns = con.columns_)\n\n\nRun the algorithm:\n\n\nCode\n# mlxtend apriori function\ndataset_apriori = ml_apriori(df_transformed, use_colnames = True, min_support = 0.5)\ndataset_apriori\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.8\n(M)\n\n\n1\n0.6\n(O)\n\n\n2\n0.8\n(P)\n\n\n3\n0.6\n(X)\n\n\n4\n0.8\n(Z)\n\n\n5\n0.6\n(O, M)\n\n\n6\n0.6\n(P, M)\n\n\n7\n0.6\n(Z, M)\n\n\n8\n0.6\n(X, P)\n\n\n9\n0.6\n(Z, P)\n\n\n\n\n\n\n\nAnd then extending past the frequent itemset results, we can run a function which will return association rules:\n\n\nCode\n# mlxtend association rules to display the rules with a 50% minimum threshold on the frequent itemsets (dataset_apriori)\nrules = ml_association(dataset_apriori, metric = 'confidence', min_threshold = 0.50)\nrules\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nzhangs_metric\n\n\n\n\n0\n(O)\n(M)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n1\n(M)\n(O)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n2\n(P)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n3\n(M)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n4\n(Z)\n(M)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n5\n(M)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n6\n(X)\n(P)\n0.6\n0.8\n0.6\n1.00\n1.2500\n0.12\ninf\n0.50\n\n\n7\n(P)\n(X)\n0.8\n0.6\n0.6\n0.75\n1.2500\n0.12\n1.6\n1.00\n\n\n8\n(Z)\n(P)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25\n\n\n9\n(P)\n(Z)\n0.8\n0.8\n0.6\n0.75\n0.9375\n-0.04\n0.8\n-0.25"
  }
]